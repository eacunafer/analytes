{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation using autoencoders\n",
    "#### Edgar Acuna\n",
    "#### July 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import  StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "%matplotlib inline\n",
    "mat = scipy.io.loadmat('C:/Users/eacun/Downloads/dataset55_release2.mat')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=mat['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49500, 1701)\n"
     ]
    }
   ],
   "source": [
    "df=mat['spectra']\n",
    "df=pd.DataFrame(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys=mat['substrateIDs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs=mat['substrateSpectra']\n",
    "subs=pd.DataFrame(subs)\n",
    "#subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "      <th>Analyte</th>\n",
       "      <th>substrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49495</th>\n",
       "      <td>0.037553</td>\n",
       "      <td>0.028473</td>\n",
       "      <td>0.036529</td>\n",
       "      <td>0.036093</td>\n",
       "      <td>0.016089</td>\n",
       "      <td>0.032855</td>\n",
       "      <td>0.029435</td>\n",
       "      <td>0.028151</td>\n",
       "      <td>0.030395</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531353</td>\n",
       "      <td>0.543420</td>\n",
       "      <td>0.535867</td>\n",
       "      <td>0.538912</td>\n",
       "      <td>0.536256</td>\n",
       "      <td>0.547188</td>\n",
       "      <td>0.539499</td>\n",
       "      <td>0.553740</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49496</th>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.032666</td>\n",
       "      <td>0.032152</td>\n",
       "      <td>0.033202</td>\n",
       "      <td>0.033766</td>\n",
       "      <td>0.031824</td>\n",
       "      <td>0.029444</td>\n",
       "      <td>0.035251</td>\n",
       "      <td>0.034372</td>\n",
       "      <td>0.031036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544204</td>\n",
       "      <td>0.544876</td>\n",
       "      <td>0.545065</td>\n",
       "      <td>0.546364</td>\n",
       "      <td>0.542205</td>\n",
       "      <td>0.539576</td>\n",
       "      <td>0.539069</td>\n",
       "      <td>0.543899</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49497</th>\n",
       "      <td>0.017774</td>\n",
       "      <td>0.024707</td>\n",
       "      <td>0.040428</td>\n",
       "      <td>0.023651</td>\n",
       "      <td>0.021570</td>\n",
       "      <td>0.033614</td>\n",
       "      <td>0.025827</td>\n",
       "      <td>0.023418</td>\n",
       "      <td>0.016353</td>\n",
       "      <td>0.044331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534933</td>\n",
       "      <td>0.549713</td>\n",
       "      <td>0.550770</td>\n",
       "      <td>0.546890</td>\n",
       "      <td>0.538551</td>\n",
       "      <td>0.528992</td>\n",
       "      <td>0.533424</td>\n",
       "      <td>0.541260</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49498</th>\n",
       "      <td>0.031123</td>\n",
       "      <td>0.029965</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.033308</td>\n",
       "      <td>0.031816</td>\n",
       "      <td>0.031922</td>\n",
       "      <td>0.033099</td>\n",
       "      <td>0.032652</td>\n",
       "      <td>0.032133</td>\n",
       "      <td>0.030951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542406</td>\n",
       "      <td>0.542487</td>\n",
       "      <td>0.540469</td>\n",
       "      <td>0.541910</td>\n",
       "      <td>0.542449</td>\n",
       "      <td>0.541800</td>\n",
       "      <td>0.539430</td>\n",
       "      <td>0.542759</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49499</th>\n",
       "      <td>0.032366</td>\n",
       "      <td>0.032454</td>\n",
       "      <td>0.032383</td>\n",
       "      <td>0.032554</td>\n",
       "      <td>0.032657</td>\n",
       "      <td>0.032594</td>\n",
       "      <td>0.032640</td>\n",
       "      <td>0.032542</td>\n",
       "      <td>0.032122</td>\n",
       "      <td>0.031636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543924</td>\n",
       "      <td>0.543932</td>\n",
       "      <td>0.543901</td>\n",
       "      <td>0.543841</td>\n",
       "      <td>0.543787</td>\n",
       "      <td>0.543787</td>\n",
       "      <td>0.543832</td>\n",
       "      <td>0.543866</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1703 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "49495  0.037553  0.028473  0.036529  0.036093  0.016089  0.032855  0.029435   \n",
       "49496  0.029870  0.032666  0.032152  0.033202  0.033766  0.031824  0.029444   \n",
       "49497  0.017774  0.024707  0.040428  0.023651  0.021570  0.033614  0.025827   \n",
       "49498  0.031123  0.029965  0.032992  0.033308  0.031816  0.031922  0.033099   \n",
       "49499  0.032366  0.032454  0.032383  0.032554  0.032657  0.032594  0.032640   \n",
       "\n",
       "              7         8         9  ...      1693      1694      1695  \\\n",
       "49495  0.028151  0.030395  0.024624  ...  0.531353  0.543420  0.535867   \n",
       "49496  0.035251  0.034372  0.031036  ...  0.544204  0.544876  0.545065   \n",
       "49497  0.023418  0.016353  0.044331  ...  0.534933  0.549713  0.550770   \n",
       "49498  0.032652  0.032133  0.030951  ...  0.542406  0.542487  0.540469   \n",
       "49499  0.032542  0.032122  0.031636  ...  0.543924  0.543932  0.543901   \n",
       "\n",
       "           1696      1697      1698      1699      1700  Analyte  substrate  \n",
       "49495  0.538912  0.536256  0.547188  0.539499  0.553740       55          8  \n",
       "49496  0.546364  0.542205  0.539576  0.539069  0.543899       55          8  \n",
       "49497  0.546890  0.538551  0.528992  0.533424  0.541260       55          8  \n",
       "49498  0.541910  0.542449  0.541800  0.539430  0.542759       55          8  \n",
       "49499  0.543841  0.543787  0.543787  0.543832  0.543866       55          8  \n",
       "\n",
       "[5 rows x 1703 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfset2=df.copy()\n",
    "dfset2['Analyte']=y\n",
    "dfset2['substrate']=ys\n",
    "dfset2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the nine substrates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "      <th>Analyte</th>\n",
       "      <th>substrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38500</th>\n",
       "      <td>0.024015</td>\n",
       "      <td>0.039861</td>\n",
       "      <td>0.034675</td>\n",
       "      <td>0.029205</td>\n",
       "      <td>0.032690</td>\n",
       "      <td>0.021443</td>\n",
       "      <td>0.015242</td>\n",
       "      <td>0.016019</td>\n",
       "      <td>0.017425</td>\n",
       "      <td>0.021906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033070</td>\n",
       "      <td>0.021220</td>\n",
       "      <td>0.038450</td>\n",
       "      <td>0.042687</td>\n",
       "      <td>0.026959</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>0.014245</td>\n",
       "      <td>0.021775</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38501</th>\n",
       "      <td>0.024314</td>\n",
       "      <td>0.024558</td>\n",
       "      <td>0.027552</td>\n",
       "      <td>0.029469</td>\n",
       "      <td>0.028968</td>\n",
       "      <td>0.027818</td>\n",
       "      <td>0.026456</td>\n",
       "      <td>0.025333</td>\n",
       "      <td>0.025255</td>\n",
       "      <td>0.024868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028214</td>\n",
       "      <td>0.027968</td>\n",
       "      <td>0.027430</td>\n",
       "      <td>0.027127</td>\n",
       "      <td>0.026760</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>0.025938</td>\n",
       "      <td>0.025846</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38502</th>\n",
       "      <td>0.020178</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.030922</td>\n",
       "      <td>0.033027</td>\n",
       "      <td>0.025272</td>\n",
       "      <td>0.020529</td>\n",
       "      <td>0.033971</td>\n",
       "      <td>0.030392</td>\n",
       "      <td>0.021519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.040675</td>\n",
       "      <td>0.020609</td>\n",
       "      <td>0.023321</td>\n",
       "      <td>0.015935</td>\n",
       "      <td>0.023401</td>\n",
       "      <td>0.032025</td>\n",
       "      <td>0.021270</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38503</th>\n",
       "      <td>0.021856</td>\n",
       "      <td>0.023824</td>\n",
       "      <td>0.021751</td>\n",
       "      <td>0.026594</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025255</td>\n",
       "      <td>0.022746</td>\n",
       "      <td>0.022709</td>\n",
       "      <td>0.024891</td>\n",
       "      <td>0.024903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025364</td>\n",
       "      <td>0.024616</td>\n",
       "      <td>0.027775</td>\n",
       "      <td>0.020910</td>\n",
       "      <td>0.022473</td>\n",
       "      <td>0.021650</td>\n",
       "      <td>0.019606</td>\n",
       "      <td>0.017557</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38504</th>\n",
       "      <td>0.022338</td>\n",
       "      <td>0.022713</td>\n",
       "      <td>0.027130</td>\n",
       "      <td>0.027351</td>\n",
       "      <td>0.026226</td>\n",
       "      <td>0.024680</td>\n",
       "      <td>0.023145</td>\n",
       "      <td>0.023026</td>\n",
       "      <td>0.021459</td>\n",
       "      <td>0.023483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024759</td>\n",
       "      <td>0.027120</td>\n",
       "      <td>0.025498</td>\n",
       "      <td>0.024313</td>\n",
       "      <td>0.024917</td>\n",
       "      <td>0.024866</td>\n",
       "      <td>0.022272</td>\n",
       "      <td>0.021763</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1703 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "38500  0.024015  0.039861  0.034675  0.029205  0.032690  0.021443  0.015242   \n",
       "38501  0.024314  0.024558  0.027552  0.029469  0.028968  0.027818  0.026456   \n",
       "38502  0.020178  0.018663  0.022727  0.030922  0.033027  0.025272  0.020529   \n",
       "38503  0.021856  0.023824  0.021751  0.026594  0.023305  0.025255  0.022746   \n",
       "38504  0.022338  0.022713  0.027130  0.027351  0.026226  0.024680  0.023145   \n",
       "\n",
       "              7         8         9  ...      1693      1694      1695  \\\n",
       "38500  0.016019  0.017425  0.021906  ...  0.033070  0.021220  0.038450   \n",
       "38501  0.025333  0.025255  0.024868  ...  0.028214  0.027968  0.027430   \n",
       "38502  0.033971  0.030392  0.021519  ...  0.027900  0.040675  0.020609   \n",
       "38503  0.022709  0.024891  0.024903  ...  0.025364  0.024616  0.027775   \n",
       "38504  0.023026  0.021459  0.023483  ...  0.024759  0.027120  0.025498   \n",
       "\n",
       "           1696      1697      1698      1699      1700  Analyte  substrate  \n",
       "38500  0.042687  0.026959  0.009430  0.014245  0.021775        1          9  \n",
       "38501  0.027127  0.026760  0.026210  0.025938  0.025846        1          9  \n",
       "38502  0.023321  0.015935  0.023401  0.032025  0.021270        1          9  \n",
       "38503  0.020910  0.022473  0.021650  0.019606  0.017557        1          9  \n",
       "38504  0.024313  0.024917  0.024866  0.022272  0.021763        1          9  \n",
       "\n",
       "[5 rows x 1703 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsub1=dfset2[dfset2['substrate']==1]\n",
    "dfsub2=dfset2[dfset2['substrate']==2]\n",
    "dfsub3=dfset2[dfset2['substrate']==3]\n",
    "dfsub4=dfset2[dfset2['substrate']==4]\n",
    "dfsub5=dfset2[dfset2['substrate']==5]\n",
    "dfsub6=dfset2[dfset2['substrate']==6]\n",
    "dfsub7=dfset2[dfset2['substrate']==7]\n",
    "dfsub8=dfset2[dfset2['substrate']==8]\n",
    "dfsub9=dfset2[dfset2['substrate']==9]\n",
    "#dfset1=pd.DataFrame(dfset1)\n",
    "df['Analyte']=y\n",
    "df['substrate']=ys\n",
    "#df.iloc[20000:20006,:]\n",
    "#yt=dfset5[\"Analyte\"]\n",
    "dfsub9.head()\n",
    "#dfset5.shape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfsub1=dfsub1.iloc[:,0:1701]\n",
    "mdfsub2=dfsub2.iloc[:,0:1701]\n",
    "mdfsub3=dfsub3.iloc[:,0:1701]\n",
    "mdfsub4=dfsub4.iloc[:,0:1701]\n",
    "mdfsub5=dfsub5.iloc[:,0:1701]\n",
    "mdfsub6=dfsub6.iloc[:,0:1701]\n",
    "mdfsub7=dfsub7.iloc[:,0:1701]\n",
    "mdfsub8=dfsub8.iloc[:,0:1701]\n",
    "mdfsub9=dfsub9.iloc[:,0:1701]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=subs.loc[0,:]\n",
    "s2=subs.loc[1,:]\n",
    "s3=subs.loc[2,:]\n",
    "s4=subs.loc[3,:]\n",
    "s5=subs.loc[4,:]\n",
    "s6=subs.loc[5,:]\n",
    "s7=subs.loc[6,:]\n",
    "s8=subs.loc[7,:]\n",
    "s9=subs.loc[8,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modsub1=mdfsub1.apply(lambda x : x -(np.sum(np.array(x)*np.array(s1))/np.sum(np.array(s1)*np.array(s1)))*s1,axis=1)\n",
    "modsub2=mdfsub2.apply(lambda x : x -(np.sum(np.array(x)*np.array(s2))/np.sum(np.array(s2)*np.array(s2)))*s2,axis=1)\n",
    "modsub3=mdfsub3.apply(lambda x : x -(np.sum(np.array(x)*np.array(s3))/np.sum(np.array(s3)*np.array(s3)))*s3,axis=1)\n",
    "modsub4=mdfsub4.apply(lambda x : x -(np.sum(np.array(x)*np.array(s4))/np.sum(np.array(s4)*np.array(s4)))*s4,axis=1)\n",
    "modsub5=mdfsub5.apply(lambda x : x -(np.sum(np.array(x)*np.array(s5))/np.sum(np.array(s5)*np.array(s5)))*s5,axis=1)\n",
    "modsub6=mdfsub6.apply(lambda x : x -(np.sum(np.array(x)*np.array(s6))/np.sum(np.array(s6)*np.array(s6)))*s6,axis=1)\n",
    "modsub7=mdfsub7.apply(lambda x : x -(np.sum(np.array(x)*np.array(s7))/np.sum(np.array(s7)*np.array(s7)))*s7,axis=1)\n",
    "modsub8=mdfsub8.apply(lambda x : x -(np.sum(np.array(x)*np.array(s8))/np.sum(np.array(s8)*np.array(s8)))*s8,axis=1)\n",
    "modsub9=mdfsub9.apply(lambda x : x -(np.sum(np.array(x)*np.array(s9))/np.sum(np.array(s9)*np.array(s9)))*s9,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data centered by substrates background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1691</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003352</td>\n",
       "      <td>-0.004829</td>\n",
       "      <td>-0.003905</td>\n",
       "      <td>-0.002224</td>\n",
       "      <td>-0.002386</td>\n",
       "      <td>-0.003332</td>\n",
       "      <td>-0.003701</td>\n",
       "      <td>-0.003463</td>\n",
       "      <td>-0.004489</td>\n",
       "      <td>-0.005034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003758</td>\n",
       "      <td>-0.002548</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>-0.011134</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.005120</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>-0.003049</td>\n",
       "      <td>-0.006294</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010454</td>\n",
       "      <td>0.004719</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>-0.001578</td>\n",
       "      <td>-0.002636</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>-0.004356</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>-0.007237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003437</td>\n",
       "      <td>-0.003414</td>\n",
       "      <td>-0.003390</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>-0.003341</td>\n",
       "      <td>-0.003316</td>\n",
       "      <td>-0.003289</td>\n",
       "      <td>-0.003263</td>\n",
       "      <td>-0.003236</td>\n",
       "      <td>-0.003209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>-0.002580</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>-0.003121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002536</td>\n",
       "      <td>-0.007373</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-0.002173</td>\n",
       "      <td>-0.007754</td>\n",
       "      <td>-0.006247</td>\n",
       "      <td>-0.003583</td>\n",
       "      <td>-0.004043</td>\n",
       "      <td>-0.002472</td>\n",
       "      <td>-0.008282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.009750</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>-0.004758</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003264</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>-0.008510</td>\n",
       "      <td>-0.000614</td>\n",
       "      <td>-0.004077</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.004247</td>\n",
       "      <td>-0.002622</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.003888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.001835  0.000878  0.000793  0.000247  0.000728  0.001072  0.002347   \n",
       "1  0.003758 -0.002548  0.006330 -0.011134  0.000164  0.005120 -0.002903   \n",
       "2  0.001106  0.001177  0.001245  0.001310  0.001369  0.001383  0.001337   \n",
       "3  0.000363  0.003034  0.003668  0.002877  0.002038  0.000546  0.002638   \n",
       "4  0.003795  0.009750  0.001765  0.005447  0.004424  0.003195  0.001814   \n",
       "\n",
       "       7         8         9     ...      1691      1692      1693      1694  \\\n",
       "0  0.000115  0.000966  0.001123  ... -0.003352 -0.004829 -0.003905 -0.002224   \n",
       "1 -0.003049 -0.006294  0.003996  ... -0.010454  0.004719 -0.009114 -0.001578   \n",
       "2  0.001292  0.001355  0.001509  ... -0.003437 -0.003414 -0.003390 -0.003366   \n",
       "3 -0.002580 -0.000733 -0.003121  ... -0.002536 -0.007373 -0.000276 -0.002173   \n",
       "4 -0.004758  0.005961  0.005176  ... -0.003264  0.001135 -0.008510 -0.000614   \n",
       "\n",
       "       1695      1696      1697      1698      1699      1700  \n",
       "0 -0.002386 -0.003332 -0.003701 -0.003463 -0.004489 -0.005034  \n",
       "1 -0.002636 -0.001219  0.000561 -0.004356  0.002053 -0.007237  \n",
       "2 -0.003341 -0.003316 -0.003289 -0.003263 -0.003236 -0.003209  \n",
       "3 -0.007754 -0.006247 -0.003583 -0.004043 -0.002472 -0.008282  \n",
       "4 -0.004077 -0.013175 -0.004247 -0.002622 -0.000083 -0.003888  \n",
       "\n",
       "[5 rows x 1701 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf=[modsub1,modsub2,modsub3,modsub4,modsub5,modsub6,modsub7,modsub8,modsub9]\n",
    "cent_subs=pd.concat(subdf)\n",
    "cent_subs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1691</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003352</td>\n",
       "      <td>-0.004829</td>\n",
       "      <td>-0.003905</td>\n",
       "      <td>-0.002224</td>\n",
       "      <td>-0.002386</td>\n",
       "      <td>-0.003332</td>\n",
       "      <td>-0.003701</td>\n",
       "      <td>-0.003463</td>\n",
       "      <td>-0.004489</td>\n",
       "      <td>-0.005034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003758</td>\n",
       "      <td>-0.002548</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>-0.011134</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.005120</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>-0.003049</td>\n",
       "      <td>-0.006294</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010454</td>\n",
       "      <td>0.004719</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>-0.001578</td>\n",
       "      <td>-0.002636</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>-0.004356</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>-0.007237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003437</td>\n",
       "      <td>-0.003414</td>\n",
       "      <td>-0.003390</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>-0.003341</td>\n",
       "      <td>-0.003316</td>\n",
       "      <td>-0.003289</td>\n",
       "      <td>-0.003263</td>\n",
       "      <td>-0.003236</td>\n",
       "      <td>-0.003209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>-0.002580</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>-0.003121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002536</td>\n",
       "      <td>-0.007373</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-0.002173</td>\n",
       "      <td>-0.007754</td>\n",
       "      <td>-0.006247</td>\n",
       "      <td>-0.003583</td>\n",
       "      <td>-0.004043</td>\n",
       "      <td>-0.002472</td>\n",
       "      <td>-0.008282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.009750</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>-0.004758</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003264</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>-0.008510</td>\n",
       "      <td>-0.000614</td>\n",
       "      <td>-0.004077</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.004247</td>\n",
       "      <td>-0.002622</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.003888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.001835  0.000878  0.000793  0.000247  0.000728  0.001072  0.002347   \n",
       "1  0.003758 -0.002548  0.006330 -0.011134  0.000164  0.005120 -0.002903   \n",
       "2  0.001106  0.001177  0.001245  0.001310  0.001369  0.001383  0.001337   \n",
       "3  0.000363  0.003034  0.003668  0.002877  0.002038  0.000546  0.002638   \n",
       "4  0.003795  0.009750  0.001765  0.005447  0.004424  0.003195  0.001814   \n",
       "\n",
       "       7         8         9     ...      1691      1692      1693      1694  \\\n",
       "0  0.000115  0.000966  0.001123  ... -0.003352 -0.004829 -0.003905 -0.002224   \n",
       "1 -0.003049 -0.006294  0.003996  ... -0.010454  0.004719 -0.009114 -0.001578   \n",
       "2  0.001292  0.001355  0.001509  ... -0.003437 -0.003414 -0.003390 -0.003366   \n",
       "3 -0.002580 -0.000733 -0.003121  ... -0.002536 -0.007373 -0.000276 -0.002173   \n",
       "4 -0.004758  0.005961  0.005176  ... -0.003264  0.001135 -0.008510 -0.000614   \n",
       "\n",
       "       1695      1696      1697      1698      1699      1700  \n",
       "0 -0.002386 -0.003332 -0.003701 -0.003463 -0.004489 -0.005034  \n",
       "1 -0.002636 -0.001219  0.000561 -0.004356  0.002053 -0.007237  \n",
       "2 -0.003341 -0.003316 -0.003289 -0.003263 -0.003236 -0.003209  \n",
       "3 -0.007754 -0.006247 -0.003583 -0.004043 -0.002472 -0.008282  \n",
       "4 -0.004077 -0.013175 -0.004247 -0.002622 -0.000083 -0.003888  \n",
       "\n",
       "[5 rows x 1701 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "b=cent_subs.iloc[:,0:1701]\n",
    "b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1691</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009607</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.005611</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017547</td>\n",
       "      <td>-0.025276</td>\n",
       "      <td>-0.020442</td>\n",
       "      <td>-0.011642</td>\n",
       "      <td>-0.012491</td>\n",
       "      <td>-0.017440</td>\n",
       "      <td>-0.019376</td>\n",
       "      <td>-0.018127</td>\n",
       "      <td>-0.023498</td>\n",
       "      <td>-0.026352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013683</td>\n",
       "      <td>-0.009277</td>\n",
       "      <td>0.023046</td>\n",
       "      <td>-0.040534</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.018640</td>\n",
       "      <td>-0.010568</td>\n",
       "      <td>-0.011098</td>\n",
       "      <td>-0.022913</td>\n",
       "      <td>0.014546</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038060</td>\n",
       "      <td>0.017181</td>\n",
       "      <td>-0.033179</td>\n",
       "      <td>-0.005745</td>\n",
       "      <td>-0.009596</td>\n",
       "      <td>-0.004439</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>-0.015857</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>-0.026345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005920</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>0.006659</td>\n",
       "      <td>0.007008</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018391</td>\n",
       "      <td>-0.018267</td>\n",
       "      <td>-0.018138</td>\n",
       "      <td>-0.018009</td>\n",
       "      <td>-0.017876</td>\n",
       "      <td>-0.017742</td>\n",
       "      <td>-0.017598</td>\n",
       "      <td>-0.017460</td>\n",
       "      <td>-0.017316</td>\n",
       "      <td>-0.017168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.014365</td>\n",
       "      <td>0.017369</td>\n",
       "      <td>0.013624</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>0.012490</td>\n",
       "      <td>-0.012215</td>\n",
       "      <td>-0.003470</td>\n",
       "      <td>-0.014777</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012008</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>-0.001307</td>\n",
       "      <td>-0.010292</td>\n",
       "      <td>-0.036719</td>\n",
       "      <td>-0.029583</td>\n",
       "      <td>-0.016964</td>\n",
       "      <td>-0.019143</td>\n",
       "      <td>-0.011705</td>\n",
       "      <td>-0.039219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014017</td>\n",
       "      <td>0.036012</td>\n",
       "      <td>0.006521</td>\n",
       "      <td>0.020120</td>\n",
       "      <td>0.016342</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.006702</td>\n",
       "      <td>-0.017573</td>\n",
       "      <td>0.022017</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012055</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>-0.031434</td>\n",
       "      <td>-0.002269</td>\n",
       "      <td>-0.015060</td>\n",
       "      <td>-0.048664</td>\n",
       "      <td>-0.015689</td>\n",
       "      <td>-0.009685</td>\n",
       "      <td>-0.000306</td>\n",
       "      <td>-0.014363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.009607  0.004595  0.004153  0.001293  0.003809  0.005611  0.012284   \n",
       "1  0.013683 -0.009277  0.023046 -0.040534  0.000598  0.018640 -0.010568   \n",
       "2  0.005920  0.006296  0.006659  0.007008  0.007325  0.007399  0.007152   \n",
       "3  0.001720  0.014365  0.017369  0.013624  0.009649  0.002587  0.012490   \n",
       "4  0.014017  0.036012  0.006521  0.020120  0.016342  0.011801  0.006702   \n",
       "\n",
       "       7         8         9     ...      1691      1692      1693      1694  \\\n",
       "0  0.000603  0.005059  0.005881  ... -0.017547 -0.025276 -0.020442 -0.011642   \n",
       "1 -0.011098 -0.022913  0.014546  ... -0.038060  0.017181 -0.033179 -0.005745   \n",
       "2  0.006912  0.007250  0.008074  ... -0.018391 -0.018267 -0.018138 -0.018009   \n",
       "3 -0.012215 -0.003470 -0.014777  ... -0.012008 -0.034912 -0.001307 -0.010292   \n",
       "4 -0.017573  0.022017  0.019119  ... -0.012055  0.004192 -0.031434 -0.002269   \n",
       "\n",
       "       1695      1696      1697      1698      1699      1700  \n",
       "0 -0.012491 -0.017440 -0.019376 -0.018127 -0.023498 -0.026352  \n",
       "1 -0.009596 -0.004439  0.002042 -0.015857  0.007474 -0.026345  \n",
       "2 -0.017876 -0.017742 -0.017598 -0.017460 -0.017316 -0.017168  \n",
       "3 -0.036719 -0.029583 -0.016964 -0.019143 -0.011705 -0.039219  \n",
       "4 -0.015060 -0.048664 -0.015689 -0.009685 -0.000306 -0.014363  \n",
       "\n",
       "[5 rows x 1701 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize(b)\n",
    "b1=b.apply(lambda x: x/(x**2).sum()**.5, axis=1)\n",
    "b1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the data for autoenconders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52029995, 0.48648253, 0.50271907, ..., 0.44043842, 0.38646427,\n",
       "        0.3660662 ],\n",
       "       [0.5404537 , 0.41828456, 0.59125601, ..., 0.45256222, 0.55435131,\n",
       "        0.36610132],\n",
       "       [0.50206616, 0.4948428 , 0.51446392, ..., 0.44400082, 0.41997146,\n",
       "        0.41494819],\n",
       "       ...,\n",
       "       [0.53347114, 0.50617401, 0.54376633, ..., 0.54671429, 0.45723571,\n",
       "        0.29587887],\n",
       "       [0.40518202, 0.42506645, 0.4327896 , ..., 0.53529242, 0.48484028,\n",
       "        0.46822106],\n",
       "       [0.59032626, 0.68304793, 0.45215277, ..., 0.25388347, 0.52949914,\n",
       "        0.43452064]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx =b1.iloc[:,0:1701]\n",
    "trainx=trainx.to_numpy()\n",
    "scaler = MinMaxScaler()\n",
    "train_x= scaler.fit_transform(trainx)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1701)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               435712    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1701)              437157    \n",
      "=================================================================\n",
      "Total params: 955,365\n",
      "Trainable params: 955,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# No of Neurons in each Layer \n",
    "nb_epoch = 50\n",
    "batch_size = 50\n",
    "input_dim = train_x.shape[1] #num of columns, 1701\n",
    "encoding_dim = 256\n",
    "hidden_dim1 = int(encoding_dim / 2)\n",
    "hidden_dim2=int(hidden_dim1/2)\n",
    "decoding_dim = 256\n",
    "learning_rate = 1e-7\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
    "encoder = Dense(hidden_dim1, activation=\"relu\")(encoder)\n",
    "encoder= Dense(hidden_dim2, activation=\"relu\")(encoder)\n",
    "decoder = Dense(hidden_dim1, activation='tanh')(encoder)\n",
    "decoder = Dense(decoding_dim, activation='relu')(decoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0777 - val_loss: 0.0658\n",
      "Epoch 2/50\n",
      "792/792 [==============================] - 5s 7ms/step - loss: 0.0560 - val_loss: 0.0541\n",
      "Epoch 3/50\n",
      "792/792 [==============================] - 5s 7ms/step - loss: 0.0451 - val_loss: 0.0409\n",
      "Epoch 4/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0339 - val_loss: 0.0322\n",
      "Epoch 5/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0287 - val_loss: 0.0281\n",
      "Epoch 6/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0240 - val_loss: 0.0228\n",
      "Epoch 7/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0207 - val_loss: 0.0196\n",
      "Epoch 8/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0181 - val_loss: 0.0175\n",
      "Epoch 9/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0156 - val_loss: 0.0147\n",
      "Epoch 10/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0140 - val_loss: 0.0133\n",
      "Epoch 11/50\n",
      "792/792 [==============================] - 5s 7ms/step - loss: 0.0127 - val_loss: 0.0123\n",
      "Epoch 12/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0118 - val_loss: 0.0109\n",
      "Epoch 13/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 14/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 15/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 16/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0100 - val_loss: 0.0096\n",
      "Epoch 17/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 18/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 19/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0098 - val_loss: 0.0095\n",
      "Epoch 20/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 21/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0094 - val_loss: 0.0091\n",
      "Epoch 22/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0093 - val_loss: 0.0090\n",
      "Epoch 23/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0093 - val_loss: 0.0093\n",
      "Epoch 24/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 25/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0093 - val_loss: 0.0092\n",
      "Epoch 26/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0093 - val_loss: 0.0089\n",
      "Epoch 27/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0091 - val_loss: 0.0088\n",
      "Epoch 28/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0088 - val_loss: 0.0085\n",
      "Epoch 29/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 30/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0084 - val_loss: 0.0081\n",
      "Epoch 31/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 32/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 33/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 34/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0084 - val_loss: 0.0087\n",
      "Epoch 35/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0084 - val_loss: 0.0085\n",
      "Epoch 36/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0084 - val_loss: 0.0080\n",
      "Epoch 37/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0081 - val_loss: 0.0078\n",
      "Epoch 38/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 39/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0081 - val_loss: 0.0083\n",
      "Epoch 40/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0081 - val_loss: 0.0083\n",
      "Epoch 41/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 42/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0079 - val_loss: 0.0081\n",
      "Epoch 43/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 44/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 45/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 46/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 47/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 48/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 49/50\n",
      "792/792 [==============================] - 6s 7ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 50/50\n",
      "792/792 [==============================] - 6s 8ms/step - loss: 0.0079 - val_loss: 0.0083\n",
      "--- 292.4273271560669 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "autoencoder.compile(optimizer='adam', loss='mse' )\n",
    "\n",
    "history = autoencoder.fit(train_x, train_x, epochs=nb_epoch,batch_size=batch_size, shuffle=True,validation_split=0.2,verbose=1)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "df_history = pd.DataFrame(history.history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49229616 0.48463395 0.5173408  ... 0.4769416  0.4452365  0.44238222]\n",
      " [0.48075932 0.47388604 0.505584   ... 0.5046321  0.4688566  0.46079794]\n",
      " [0.49475998 0.48469663 0.51729226 ... 0.47424895 0.4415585  0.4370555 ]\n",
      " ...\n",
      " [0.48139054 0.47667208 0.50225794 ... 0.5120087  0.4653228  0.45670986]\n",
      " [0.45317185 0.44738954 0.48916566 ... 0.5135983  0.46832824 0.4652983 ]\n",
      " [0.48621035 0.48038408 0.50995445 ... 0.50247216 0.47839332 0.46677613]]\n"
     ]
    }
   ],
   "source": [
    "predictions = autoencoder.predict(train_x)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1691</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.492296</td>\n",
       "      <td>0.484634</td>\n",
       "      <td>0.517341</td>\n",
       "      <td>0.506389</td>\n",
       "      <td>0.484744</td>\n",
       "      <td>0.481265</td>\n",
       "      <td>0.489983</td>\n",
       "      <td>0.487930</td>\n",
       "      <td>0.484355</td>\n",
       "      <td>0.554140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402409</td>\n",
       "      <td>0.497639</td>\n",
       "      <td>0.446948</td>\n",
       "      <td>0.441163</td>\n",
       "      <td>0.402391</td>\n",
       "      <td>0.455650</td>\n",
       "      <td>0.453345</td>\n",
       "      <td>0.476942</td>\n",
       "      <td>0.445237</td>\n",
       "      <td>0.442382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.480759</td>\n",
       "      <td>0.473886</td>\n",
       "      <td>0.505584</td>\n",
       "      <td>0.489864</td>\n",
       "      <td>0.467874</td>\n",
       "      <td>0.468942</td>\n",
       "      <td>0.476129</td>\n",
       "      <td>0.474600</td>\n",
       "      <td>0.472143</td>\n",
       "      <td>0.539409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426597</td>\n",
       "      <td>0.521527</td>\n",
       "      <td>0.467086</td>\n",
       "      <td>0.469160</td>\n",
       "      <td>0.415806</td>\n",
       "      <td>0.485761</td>\n",
       "      <td>0.480252</td>\n",
       "      <td>0.504632</td>\n",
       "      <td>0.468857</td>\n",
       "      <td>0.460798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.494760</td>\n",
       "      <td>0.484697</td>\n",
       "      <td>0.517292</td>\n",
       "      <td>0.506938</td>\n",
       "      <td>0.485008</td>\n",
       "      <td>0.481888</td>\n",
       "      <td>0.491521</td>\n",
       "      <td>0.490463</td>\n",
       "      <td>0.486614</td>\n",
       "      <td>0.557285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399195</td>\n",
       "      <td>0.495263</td>\n",
       "      <td>0.444294</td>\n",
       "      <td>0.436953</td>\n",
       "      <td>0.400455</td>\n",
       "      <td>0.452804</td>\n",
       "      <td>0.449776</td>\n",
       "      <td>0.474249</td>\n",
       "      <td>0.441559</td>\n",
       "      <td>0.437055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.489838</td>\n",
       "      <td>0.483023</td>\n",
       "      <td>0.516647</td>\n",
       "      <td>0.505553</td>\n",
       "      <td>0.483339</td>\n",
       "      <td>0.482320</td>\n",
       "      <td>0.490332</td>\n",
       "      <td>0.486443</td>\n",
       "      <td>0.484766</td>\n",
       "      <td>0.551338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401623</td>\n",
       "      <td>0.501924</td>\n",
       "      <td>0.449276</td>\n",
       "      <td>0.444409</td>\n",
       "      <td>0.407750</td>\n",
       "      <td>0.453941</td>\n",
       "      <td>0.457766</td>\n",
       "      <td>0.477491</td>\n",
       "      <td>0.446515</td>\n",
       "      <td>0.441742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.488070</td>\n",
       "      <td>0.482602</td>\n",
       "      <td>0.509153</td>\n",
       "      <td>0.503323</td>\n",
       "      <td>0.482846</td>\n",
       "      <td>0.475292</td>\n",
       "      <td>0.482107</td>\n",
       "      <td>0.480994</td>\n",
       "      <td>0.481038</td>\n",
       "      <td>0.552155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421064</td>\n",
       "      <td>0.525189</td>\n",
       "      <td>0.467158</td>\n",
       "      <td>0.465159</td>\n",
       "      <td>0.425917</td>\n",
       "      <td>0.471404</td>\n",
       "      <td>0.479251</td>\n",
       "      <td>0.499817</td>\n",
       "      <td>0.463592</td>\n",
       "      <td>0.460260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.492296  0.484634  0.517341  0.506389  0.484744  0.481265  0.489983   \n",
       "1  0.480759  0.473886  0.505584  0.489864  0.467874  0.468942  0.476129   \n",
       "2  0.494760  0.484697  0.517292  0.506938  0.485008  0.481888  0.491521   \n",
       "3  0.489838  0.483023  0.516647  0.505553  0.483339  0.482320  0.490332   \n",
       "4  0.488070  0.482602  0.509153  0.503323  0.482846  0.475292  0.482107   \n",
       "\n",
       "       7         8         9     ...      1691      1692      1693      1694  \\\n",
       "0  0.487930  0.484355  0.554140  ...  0.402409  0.497639  0.446948  0.441163   \n",
       "1  0.474600  0.472143  0.539409  ...  0.426597  0.521527  0.467086  0.469160   \n",
       "2  0.490463  0.486614  0.557285  ...  0.399195  0.495263  0.444294  0.436953   \n",
       "3  0.486443  0.484766  0.551338  ...  0.401623  0.501924  0.449276  0.444409   \n",
       "4  0.480994  0.481038  0.552155  ...  0.421064  0.525189  0.467158  0.465159   \n",
       "\n",
       "       1695      1696      1697      1698      1699      1700  \n",
       "0  0.402391  0.455650  0.453345  0.476942  0.445237  0.442382  \n",
       "1  0.415806  0.485761  0.480252  0.504632  0.468857  0.460798  \n",
       "2  0.400455  0.452804  0.449776  0.474249  0.441559  0.437055  \n",
       "3  0.407750  0.453941  0.457766  0.477491  0.446515  0.441742  \n",
       "4  0.425917  0.471404  0.479251  0.499817  0.463592  0.460260  \n",
       "\n",
       "[5 rows x 1701 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=pd.DataFrame(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "augdata=pd.concat([train_x,predictions],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99000, 1701)\n"
     ]
    }
   ],
   "source": [
    "print(augdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=np.array(dfsub1['Analyte'].values.tolist())\n",
    "a2=np.array(dfsub2['Analyte'].values.tolist())\n",
    "a3=np.array(dfsub3['Analyte'].values.tolist())\n",
    "a4=np.array(dfsub4['Analyte'].values.tolist())\n",
    "a5=np.array(dfsub5['Analyte'].values.tolist())\n",
    "a6=np.array(dfsub6['Analyte'].values.tolist())\n",
    "a7=np.array(dfsub7['Analyte'].values.tolist())\n",
    "a8=np.array(dfsub8['Analyte'].values.tolist())\n",
    "a9=np.array(dfsub9['Analyte'].values.tolist())\n",
    "y=np.concatenate((a1,a2,a3,a4,a5,a6,a7,a8,a9),axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "augy=np.concatenate((a1,a2,a3,a4,a5,a6,a7,a8,a9,a1,a2,a3,a4,a5,a6,a7,a8,a9),axis=None)\n",
    "augy=pd.DataFrame(augy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "      <th>Analyte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98995</th>\n",
       "      <td>0.481081</td>\n",
       "      <td>0.473522</td>\n",
       "      <td>0.498453</td>\n",
       "      <td>0.488630</td>\n",
       "      <td>0.466533</td>\n",
       "      <td>0.467711</td>\n",
       "      <td>0.446872</td>\n",
       "      <td>0.468877</td>\n",
       "      <td>0.458566</td>\n",
       "      <td>0.528123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548638</td>\n",
       "      <td>0.488134</td>\n",
       "      <td>0.505453</td>\n",
       "      <td>0.462950</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.516031</td>\n",
       "      <td>0.535510</td>\n",
       "      <td>0.500633</td>\n",
       "      <td>0.482448</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98996</th>\n",
       "      <td>0.463340</td>\n",
       "      <td>0.468131</td>\n",
       "      <td>0.502735</td>\n",
       "      <td>0.480151</td>\n",
       "      <td>0.459467</td>\n",
       "      <td>0.472508</td>\n",
       "      <td>0.455706</td>\n",
       "      <td>0.465760</td>\n",
       "      <td>0.460141</td>\n",
       "      <td>0.525835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498857</td>\n",
       "      <td>0.454841</td>\n",
       "      <td>0.439482</td>\n",
       "      <td>0.400011</td>\n",
       "      <td>0.468502</td>\n",
       "      <td>0.479060</td>\n",
       "      <td>0.490411</td>\n",
       "      <td>0.446372</td>\n",
       "      <td>0.442844</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98997</th>\n",
       "      <td>0.481391</td>\n",
       "      <td>0.476672</td>\n",
       "      <td>0.502258</td>\n",
       "      <td>0.480695</td>\n",
       "      <td>0.456560</td>\n",
       "      <td>0.468710</td>\n",
       "      <td>0.445494</td>\n",
       "      <td>0.474938</td>\n",
       "      <td>0.459544</td>\n",
       "      <td>0.526656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511075</td>\n",
       "      <td>0.467222</td>\n",
       "      <td>0.457888</td>\n",
       "      <td>0.415753</td>\n",
       "      <td>0.491956</td>\n",
       "      <td>0.493354</td>\n",
       "      <td>0.512009</td>\n",
       "      <td>0.465323</td>\n",
       "      <td>0.456710</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98998</th>\n",
       "      <td>0.453172</td>\n",
       "      <td>0.447390</td>\n",
       "      <td>0.489166</td>\n",
       "      <td>0.453848</td>\n",
       "      <td>0.434457</td>\n",
       "      <td>0.454864</td>\n",
       "      <td>0.430835</td>\n",
       "      <td>0.457955</td>\n",
       "      <td>0.449316</td>\n",
       "      <td>0.514599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520115</td>\n",
       "      <td>0.474712</td>\n",
       "      <td>0.466283</td>\n",
       "      <td>0.414904</td>\n",
       "      <td>0.490324</td>\n",
       "      <td>0.502745</td>\n",
       "      <td>0.513598</td>\n",
       "      <td>0.468328</td>\n",
       "      <td>0.465298</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98999</th>\n",
       "      <td>0.486210</td>\n",
       "      <td>0.480384</td>\n",
       "      <td>0.509954</td>\n",
       "      <td>0.497820</td>\n",
       "      <td>0.469597</td>\n",
       "      <td>0.477264</td>\n",
       "      <td>0.475185</td>\n",
       "      <td>0.485645</td>\n",
       "      <td>0.472842</td>\n",
       "      <td>0.531931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528153</td>\n",
       "      <td>0.478691</td>\n",
       "      <td>0.475511</td>\n",
       "      <td>0.432740</td>\n",
       "      <td>0.486215</td>\n",
       "      <td>0.500474</td>\n",
       "      <td>0.502472</td>\n",
       "      <td>0.478393</td>\n",
       "      <td>0.466776</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1702 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "98995  0.481081  0.473522  0.498453  0.488630  0.466533  0.467711  0.446872   \n",
       "98996  0.463340  0.468131  0.502735  0.480151  0.459467  0.472508  0.455706   \n",
       "98997  0.481391  0.476672  0.502258  0.480695  0.456560  0.468710  0.445494   \n",
       "98998  0.453172  0.447390  0.489166  0.453848  0.434457  0.454864  0.430835   \n",
       "98999  0.486210  0.480384  0.509954  0.497820  0.469597  0.477264  0.475185   \n",
       "\n",
       "              7         8         9  ...      1692      1693      1694  \\\n",
       "98995  0.468877  0.458566  0.528123  ...  0.548638  0.488134  0.505453   \n",
       "98996  0.465760  0.460141  0.525835  ...  0.498857  0.454841  0.439482   \n",
       "98997  0.474938  0.459544  0.526656  ...  0.511075  0.467222  0.457888   \n",
       "98998  0.457955  0.449316  0.514599  ...  0.520115  0.474712  0.466283   \n",
       "98999  0.485645  0.472842  0.531931  ...  0.528153  0.478691  0.475511   \n",
       "\n",
       "           1695      1696      1697      1698      1699      1700  Analyte  \n",
       "98995  0.462950  0.512900  0.516031  0.535510  0.500633  0.482448       55  \n",
       "98996  0.400011  0.468502  0.479060  0.490411  0.446372  0.442844       55  \n",
       "98997  0.415753  0.491956  0.493354  0.512009  0.465323  0.456710       55  \n",
       "98998  0.414904  0.490324  0.502745  0.513598  0.468328  0.465298       55  \n",
       "98999  0.432740  0.486215  0.500474  0.502472  0.478393  0.466776       55  \n",
       "\n",
       "[5 rows x 1702 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augdata['Analyte']=augy\n",
    "augdata.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
      "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
      "       52, 53, 54, 55]), array([1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800,\n",
      "       1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800,\n",
      "       1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800,\n",
      "       1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800,\n",
      "       1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800],\n",
      "      dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "y=augy.to_numpy()\n",
    "print(np.unique(y,return_counts=True))\n",
    "#print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, accuracy=85.17%\n",
      "time 75.50481414794922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# We will find by inspection the best k according to the classifier accuracy on the test set\n",
    "accuracies = []\n",
    "X=augdata.iloc[:,0:1701]\n",
    "# We will find by inspection the best k according to the classifier accuracy on the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=0)\n",
    "start_time=time.time()\n",
    "for k in range(1, 2, 2):\n",
    "    # Entrenar el clasificador  con el valor actual de  `k`\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar los modelos e imprimiendo sus predicciones\n",
    "    score = neigh.score(X_test, y_test)\n",
    "    print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "    accuracies.append(score)\n",
    "print(\"time\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=augdata.iloc[:,0:1701]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size=0.2,random_state=0)\n",
    "X_train=pd.DataFrame(X_train)\n",
    "X_test=pd.DataFrame(X_test)\n",
    "X_train.to_csv(\"c:/Users/eacun/NRL/NRLDataset/train/Subs/X_train.csv\",index=False,header=False)\n",
    "X_test.to_csv(\"c:/Users/eacun/NRL/NRLDataset/test/Subs/X_test.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('c:/Users/eacun/NRL/NRLDataset/train/y_train.csv', Y_train) \n",
    "np.savetxt('c:/Users/eacun/NRL/NRLDataset/test/y_test.csv', Y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is= 0.7803535353535354\n",
      "--- 5423.509306192398 seconds ---\n",
      "F1-score is= 0.8060200201258876\n",
      "precision= 0.8537518307564993\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "import time\n",
    "#y=dfsub1['Analyte']\n",
    "#print(y.value_counts())\n",
    "X=augdata.iloc[:,0:1701]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#clf=svm.svmSVC(max_iter=2000)\n",
    "start_time = time.time()\n",
    "clf=svm.SVC()\n",
    "clf.fit(X_train, y_train) \n",
    "print(\"The accuracy is=\",clf.score(X_test,y_test))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "predictions = clf.predict(X_test)\n",
    "#prob4=pd.DataFrame(clf.predict_proba(X_test))\n",
    "#a=prob4.max(axis=1)\n",
    "#print('Probability of classification',(a[a>.50].shape[0])/prob2.shape[0])\n",
    "print(\"F1-score is=\",f1_score(y_test,predictions, average=\"weighted\"))\n",
    "print(\"precision=\",precision_score(y_test,predictions,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOt RUn more tha 40 iterations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "X=augdata.iloc[:,0:1701]\n",
    "#model.fit(X,y)\n",
    "#model.score(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=0)\n",
    "start_time = time.time()\n",
    "model = LogisticRegression(solver=\"newton-cg\",multi_class=\"multinomial\",max_iter=40)\n",
    "model.fit(X_train,y_train)\n",
    "#Calculating  metrics of prediction\n",
    "predictions = model.predict(X_test)\n",
    "#print(predictions)\n",
    "#print(\"F1-score is=\",f1_score(y_test,predictions, average=\"weighted\"))\n",
    "#print(\"precision=\",precision_score(y_test,predictions,average=\"weighted\"))\n",
    "print(\"The accuracy is=\",model.score(X_test,y_test))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import tempfile\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, roc_auc_score\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration options\n",
    "feature_vector_length = 1701\n",
    "num_classes = 55\n",
    "x=augdata.iloc[:,0:1701].to_numpy()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size=0.2,random_state=0)\n",
    "# Convert target classes to categorical ones\n",
    "ytrain=Y_train-1\n",
    "ytest=Y_test-1\n",
    "Y_train = to_categorical(ytrain, num_classes)\n",
    "Y_test = to_categorical(ytest, num_classes)\n",
    "print('Train dimension:')\n",
    "print(X_train.shape)\n",
    "print('Test dimension:')\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input shape\n",
    "input_shape = (feature_vector_length,)\n",
    "print(f'Feature shape: {input_shape}')\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "#model.add(Dropout(0.3, input_shape=input_shape))\n",
    "model.add(Dense(300, input_shape=input_shape, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model and start training\n",
    "import time\n",
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "baseline_history=model.fit(X_train, Y_train, epochs=100, batch_size=150, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training time:---  %s seconds ---\" % (time.time() - start_time))\n",
    "#Test the model after training\n",
    "start_time=time.time()\n",
    "test_results = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(test_results)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.datasets import load_iris\n",
    "from numpy import unique\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=augdata.iloc[:,0:1701].to_numpy()\n",
    "x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "print(x.shape)\n",
    "y=y-1\n",
    "#print(unique(y))\n",
    "#print(unique(y).sum())\n",
    "\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(32, 3, activation=\"relu\", input_shape=(1701,1)))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(32, 3, activation=\"relu\", input_shape=(1701,1)))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(55, activation = 'softmax'))\n",
    "start_time = time.time()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "model.summary()\n",
    "baseline_history=model.fit(xtrain, ytrain, epochs=30, batch_size=256, verbose=1, validation_split=0.2)\n",
    "#model.fit(xtrain, ytrain, batch_size=256,epochs=25,  validation_split=.2, verbose=1)\n",
    "acc = model.evaluate(xtrain, ytrain)\n",
    "print(\"Loss:\", acc[0], \" Accuracy:\", acc[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#Test the model after training\n",
    "start_time=time.time()\n",
    "test_results = model.evaluate(xtest, ytest, verbose=1)\n",
    "print(test_results)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
