{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf7e561",
   "metadata": {},
   "source": [
    "### NRL Large Dataset\n",
    "### June 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73e6908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mat = scipy.io.loadmat('C:/Users/eacun/Downloads/dataset55_release2.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7029a604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('__header__', b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Wed Jul 22 14:13:11 2020'), ('__version__', '1.0'), ('__globals__', []), ('addedNoisePercent', array([[0.1],\n",
       "       [0.5],\n",
       "       [0. ],\n",
       "       ...,\n",
       "       [1. ],\n",
       "       [0.1],\n",
       "       [0. ]])), ('labels', array([[ 1],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       ...,\n",
       "       [55],\n",
       "       [55],\n",
       "       [55]], dtype=uint8)), ('massLoadings', array([[90.48134734],\n",
       "       [90.48134734],\n",
       "       [90.48134734],\n",
       "       ...,\n",
       "       [93.56603952],\n",
       "       [93.56603952],\n",
       "       [93.56603952]])), ('spectra', array([[0.01684698, 0.01575019, 0.01553012, ..., 0.01631328, 0.01523777,\n",
       "        0.01464116],\n",
       "       [0.01862416, 0.01217942, 0.02092375, ..., 0.01522821, 0.02158774,\n",
       "        0.01224737],\n",
       "       [0.01634829, 0.01627708, 0.01620733, ..., 0.01681628, 0.01679286,\n",
       "        0.0167685 ],\n",
       "       ...,\n",
       "       [0.01777366, 0.02470746, 0.04042846, ..., 0.52899222, 0.53342443,\n",
       "        0.54125978],\n",
       "       [0.03112273, 0.02996503, 0.03299245, ..., 0.54179967, 0.53942979,\n",
       "        0.54275879],\n",
       "       [0.03236588, 0.0324536 , 0.03238276, ..., 0.54378712, 0.54383177,\n",
       "        0.54386556]])), ('substrateIDs', array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [8],\n",
       "       [8],\n",
       "       [8]], dtype=uint8)), ('substrateSpectra', array([[0.008192, 0.008116, 0.008042, ..., 0.010792, 0.010765, 0.010737],\n",
       "       [0.035105, 0.035617, 0.036458, ..., 0.041036, 0.041051, 0.04107 ],\n",
       "       [0.585617, 0.585672, 0.585179, ..., 0.559301, 0.559264, 0.55923 ],\n",
       "       ...,\n",
       "       [0.035515, 0.035251, 0.034737, ..., 0.247776, 0.246856, 0.245963],\n",
       "       [0.031312, 0.031438, 0.031556, ..., 0.569884, 0.56993 , 0.569966],\n",
       "       [0.00088 , 0.001132, 0.004165, ..., 0.008746, 0.008464, 0.008364]]))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aefc9570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       ...,\n",
       "       [55],\n",
       "       [55],\n",
       "       [55]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=mat['labels']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0e0d99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49500, 1701)\n"
     ]
    }
   ],
   "source": [
    "df=mat['spectra']\n",
    "df=pd.DataFrame(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84abd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys=mat['substrateIDs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ecceaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1691</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008192</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>0.007970</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.007830</td>\n",
       "      <td>0.007763</td>\n",
       "      <td>0.007697</td>\n",
       "      <td>0.007633</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010966</td>\n",
       "      <td>0.010943</td>\n",
       "      <td>0.010919</td>\n",
       "      <td>0.010895</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010845</td>\n",
       "      <td>0.010818</td>\n",
       "      <td>0.010792</td>\n",
       "      <td>0.010765</td>\n",
       "      <td>0.010737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.035105</td>\n",
       "      <td>0.035617</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>0.037003</td>\n",
       "      <td>0.037084</td>\n",
       "      <td>0.036102</td>\n",
       "      <td>0.035552</td>\n",
       "      <td>0.035033</td>\n",
       "      <td>0.034687</td>\n",
       "      <td>0.034424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040834</td>\n",
       "      <td>0.040778</td>\n",
       "      <td>0.040760</td>\n",
       "      <td>0.040794</td>\n",
       "      <td>0.040865</td>\n",
       "      <td>0.040946</td>\n",
       "      <td>0.041008</td>\n",
       "      <td>0.041036</td>\n",
       "      <td>0.041051</td>\n",
       "      <td>0.041070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.585617</td>\n",
       "      <td>0.585672</td>\n",
       "      <td>0.585179</td>\n",
       "      <td>0.584104</td>\n",
       "      <td>0.585759</td>\n",
       "      <td>0.587581</td>\n",
       "      <td>0.588336</td>\n",
       "      <td>0.589407</td>\n",
       "      <td>0.590642</td>\n",
       "      <td>0.591676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559620</td>\n",
       "      <td>0.559569</td>\n",
       "      <td>0.559520</td>\n",
       "      <td>0.559472</td>\n",
       "      <td>0.559426</td>\n",
       "      <td>0.559382</td>\n",
       "      <td>0.559341</td>\n",
       "      <td>0.559301</td>\n",
       "      <td>0.559264</td>\n",
       "      <td>0.559230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026414</td>\n",
       "      <td>0.026214</td>\n",
       "      <td>0.026014</td>\n",
       "      <td>0.025770</td>\n",
       "      <td>0.025449</td>\n",
       "      <td>0.025250</td>\n",
       "      <td>0.025171</td>\n",
       "      <td>0.025119</td>\n",
       "      <td>0.025165</td>\n",
       "      <td>0.025351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546925</td>\n",
       "      <td>0.547225</td>\n",
       "      <td>0.547537</td>\n",
       "      <td>0.547909</td>\n",
       "      <td>0.548301</td>\n",
       "      <td>0.548652</td>\n",
       "      <td>0.548971</td>\n",
       "      <td>0.549233</td>\n",
       "      <td>0.549387</td>\n",
       "      <td>0.549484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011060</td>\n",
       "      <td>0.011381</td>\n",
       "      <td>0.011618</td>\n",
       "      <td>0.011406</td>\n",
       "      <td>0.010922</td>\n",
       "      <td>0.010713</td>\n",
       "      <td>0.010692</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.010722</td>\n",
       "      <td>0.010908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023413</td>\n",
       "      <td>0.023356</td>\n",
       "      <td>0.023243</td>\n",
       "      <td>0.023115</td>\n",
       "      <td>0.023055</td>\n",
       "      <td>0.023158</td>\n",
       "      <td>0.023328</td>\n",
       "      <td>0.023359</td>\n",
       "      <td>0.023251</td>\n",
       "      <td>0.023180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.064528</td>\n",
       "      <td>0.064613</td>\n",
       "      <td>0.064131</td>\n",
       "      <td>0.063398</td>\n",
       "      <td>0.062792</td>\n",
       "      <td>0.062214</td>\n",
       "      <td>0.061542</td>\n",
       "      <td>0.060586</td>\n",
       "      <td>0.059307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277668</td>\n",
       "      <td>0.277010</td>\n",
       "      <td>0.276353</td>\n",
       "      <td>0.275671</td>\n",
       "      <td>0.274983</td>\n",
       "      <td>0.274299</td>\n",
       "      <td>0.273613</td>\n",
       "      <td>0.272943</td>\n",
       "      <td>0.272329</td>\n",
       "      <td>0.271673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.035515</td>\n",
       "      <td>0.035251</td>\n",
       "      <td>0.034737</td>\n",
       "      <td>0.034093</td>\n",
       "      <td>0.033518</td>\n",
       "      <td>0.033043</td>\n",
       "      <td>0.032536</td>\n",
       "      <td>0.032043</td>\n",
       "      <td>0.031658</td>\n",
       "      <td>0.031341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253511</td>\n",
       "      <td>0.252787</td>\n",
       "      <td>0.252041</td>\n",
       "      <td>0.251235</td>\n",
       "      <td>0.250392</td>\n",
       "      <td>0.249560</td>\n",
       "      <td>0.248698</td>\n",
       "      <td>0.247776</td>\n",
       "      <td>0.246856</td>\n",
       "      <td>0.245963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.031312</td>\n",
       "      <td>0.031438</td>\n",
       "      <td>0.031556</td>\n",
       "      <td>0.031777</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031608</td>\n",
       "      <td>0.031588</td>\n",
       "      <td>0.031441</td>\n",
       "      <td>0.031011</td>\n",
       "      <td>0.030551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569969</td>\n",
       "      <td>0.569996</td>\n",
       "      <td>0.570024</td>\n",
       "      <td>0.570030</td>\n",
       "      <td>0.570001</td>\n",
       "      <td>0.569943</td>\n",
       "      <td>0.569884</td>\n",
       "      <td>0.569884</td>\n",
       "      <td>0.569930</td>\n",
       "      <td>0.569966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.005633</td>\n",
       "      <td>0.004667</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010077</td>\n",
       "      <td>0.010496</td>\n",
       "      <td>0.010808</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.009687</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.008364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 1701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.008192  0.008116  0.008042  0.007970  0.007899  0.007830  0.007763   \n",
       "1  0.035105  0.035617  0.036458  0.037003  0.037084  0.036102  0.035552   \n",
       "2  0.585617  0.585672  0.585179  0.584104  0.585759  0.587581  0.588336   \n",
       "3  0.026414  0.026214  0.026014  0.025770  0.025449  0.025250  0.025171   \n",
       "4  0.011060  0.011381  0.011618  0.011406  0.010922  0.010713  0.010692   \n",
       "5  0.064400  0.064528  0.064613  0.064131  0.063398  0.062792  0.062214   \n",
       "6  0.035515  0.035251  0.034737  0.034093  0.033518  0.033043  0.032536   \n",
       "7  0.031312  0.031438  0.031556  0.031777  0.031780  0.031608  0.031588   \n",
       "8  0.000880  0.001132  0.004165  0.006112  0.005633  0.004667  0.003701   \n",
       "\n",
       "       7         8         9     ...      1691      1692      1693      1694  \\\n",
       "0  0.007697  0.007633  0.007570  ...  0.010966  0.010943  0.010919  0.010895   \n",
       "1  0.035033  0.034687  0.034424  ...  0.040834  0.040778  0.040760  0.040794   \n",
       "2  0.589407  0.590642  0.591676  ...  0.559620  0.559569  0.559520  0.559472   \n",
       "3  0.025119  0.025165  0.025351  ...  0.546925  0.547225  0.547537  0.547909   \n",
       "4  0.010628  0.010722  0.010908  ...  0.023413  0.023356  0.023243  0.023115   \n",
       "5  0.061542  0.060586  0.059307  ...  0.277668  0.277010  0.276353  0.275671   \n",
       "6  0.032043  0.031658  0.031341  ...  0.253511  0.252787  0.252041  0.251235   \n",
       "7  0.031441  0.031011  0.030551  ...  0.569969  0.569996  0.570024  0.570030   \n",
       "8  0.002964  0.002877  0.002152  ...  0.010077  0.010496  0.010808  0.010552   \n",
       "\n",
       "       1695      1696      1697      1698      1699      1700  \n",
       "0  0.010870  0.010845  0.010818  0.010792  0.010765  0.010737  \n",
       "1  0.040865  0.040946  0.041008  0.041036  0.041051  0.041070  \n",
       "2  0.559426  0.559382  0.559341  0.559301  0.559264  0.559230  \n",
       "3  0.548301  0.548652  0.548971  0.549233  0.549387  0.549484  \n",
       "4  0.023055  0.023158  0.023328  0.023359  0.023251  0.023180  \n",
       "5  0.274983  0.274299  0.273613  0.272943  0.272329  0.271673  \n",
       "6  0.250392  0.249560  0.248698  0.247776  0.246856  0.245963  \n",
       "7  0.570001  0.569943  0.569884  0.569884  0.569930  0.569966  \n",
       "8  0.010001  0.009687  0.009309  0.008746  0.008464  0.008364  \n",
       "\n",
       "[9 rows x 1701 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs=mat['substrateSpectra']\n",
    "subs=pd.DataFrame(subs)\n",
    "subs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d862604",
   "metadata": {},
   "source": [
    "### Excluding the hard analytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8dfa5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45900, 1703)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfset2=df.copy()\n",
    "dfset2['Analyte']=y\n",
    "dfset2['substrate']=ys\n",
    "hard=[27,30,40,55]\n",
    "dfset5=dfset2[-dfset2[\"Analyte\"].isin(hard)]\n",
    "dfsub1=dfset5[dfset5['substrate']==1]\n",
    "dfsub2=dfset5[dfset5['substrate']==2]\n",
    "dfsub3=dfset5[dfset5['substrate']==3]\n",
    "dfsub4=dfset5[dfset5['substrate']==4]\n",
    "dfsub5=dfset5[dfset5['substrate']==5]\n",
    "dfsub6=dfset5[dfset5['substrate']==6]\n",
    "dfsub7=dfset5[dfset5['substrate']==7]\n",
    "dfsub8=dfset5[dfset5['substrate']==8]\n",
    "dfsub9=dfset5[dfset5['substrate']==9]\n",
    "#dfset1=pd.DataFrame(dfset1)\n",
    "#yt=dfset5[\"Analyte\"]\n",
    "dfset5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d5903",
   "metadata": {},
   "source": [
    "### Substrate's effect correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6532fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfsub1=dfsub1.iloc[:,0:1701]\n",
    "mdfsub2=dfsub2.iloc[:,0:1701]\n",
    "mdfsub3=dfsub3.iloc[:,0:1701]\n",
    "mdfsub4=dfsub4.iloc[:,0:1701]\n",
    "mdfsub5=dfsub5.iloc[:,0:1701]\n",
    "mdfsub6=dfsub6.iloc[:,0:1701]\n",
    "mdfsub7=dfsub7.iloc[:,0:1701]\n",
    "mdfsub8=dfsub8.iloc[:,0:1701]\n",
    "mdfsub9=dfsub9.iloc[:,0:1701]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5628ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1701)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf975ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=subs.loc[0,:]\n",
    "s2=subs.loc[1,:]\n",
    "s3=subs.loc[2,:]\n",
    "s4=subs.loc[3,:]\n",
    "s5=subs.loc[4,:]\n",
    "s6=subs.loc[5,:]\n",
    "s7=subs.loc[6,:]\n",
    "s8=subs.loc[7,:]\n",
    "s9=subs.loc[8,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c6c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "modsub1=mdfsub1.apply(lambda x : x -(np.sum(np.array(x)*np.array(s1))/np.sum(np.array(s1)*np.array(s1)))*s1,axis=1)\n",
    "modsub2=mdfsub2.apply(lambda x : x -(np.sum(np.array(x)*np.array(s2))/np.sum(np.array(s2)*np.array(s2)))*s2,axis=1)\n",
    "modsub3=mdfsub3.apply(lambda x : x -(np.sum(np.array(x)*np.array(s3))/np.sum(np.array(s3)*np.array(s3)))*s3,axis=1)\n",
    "modsub4=mdfsub4.apply(lambda x : x -(np.sum(np.array(x)*np.array(s4))/np.sum(np.array(s4)*np.array(s4)))*s4,axis=1)\n",
    "modsub5=mdfsub5.apply(lambda x : x -(np.sum(np.array(x)*np.array(s5))/np.sum(np.array(s5)*np.array(s5)))*s5,axis=1)\n",
    "modsub6=mdfsub6.apply(lambda x : x -(np.sum(np.array(x)*np.array(s6))/np.sum(np.array(s6)*np.array(s6)))*s6,axis=1)\n",
    "modsub7=mdfsub7.apply(lambda x : x -(np.sum(np.array(x)*np.array(s7))/np.sum(np.array(s7)*np.array(s7)))*s7,axis=1)\n",
    "modsub8=mdfsub8.apply(lambda x : x -(np.sum(np.array(x)*np.array(s8))/np.sum(np.array(s8)*np.array(s8)))*s8,axis=1)\n",
    "modsub9=mdfsub9.apply(lambda x : x -(np.sum(np.array(x)*np.array(s9))/np.sum(np.array(s9)*np.array(s9)))*s9,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e7b8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1691</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003352</td>\n",
       "      <td>-0.004829</td>\n",
       "      <td>-0.003905</td>\n",
       "      <td>-0.002224</td>\n",
       "      <td>-0.002386</td>\n",
       "      <td>-0.003332</td>\n",
       "      <td>-0.003701</td>\n",
       "      <td>-0.003463</td>\n",
       "      <td>-0.004489</td>\n",
       "      <td>-0.005034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003758</td>\n",
       "      <td>-0.002548</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>-0.011134</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.005120</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>-0.003049</td>\n",
       "      <td>-0.006294</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010454</td>\n",
       "      <td>0.004719</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>-0.001578</td>\n",
       "      <td>-0.002636</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>-0.004356</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>-0.007237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003437</td>\n",
       "      <td>-0.003414</td>\n",
       "      <td>-0.003390</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>-0.003341</td>\n",
       "      <td>-0.003316</td>\n",
       "      <td>-0.003289</td>\n",
       "      <td>-0.003263</td>\n",
       "      <td>-0.003236</td>\n",
       "      <td>-0.003209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>-0.002580</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>-0.003121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002536</td>\n",
       "      <td>-0.007373</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-0.002173</td>\n",
       "      <td>-0.007754</td>\n",
       "      <td>-0.006247</td>\n",
       "      <td>-0.003583</td>\n",
       "      <td>-0.004043</td>\n",
       "      <td>-0.002472</td>\n",
       "      <td>-0.008282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.009750</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>-0.004758</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003264</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>-0.008510</td>\n",
       "      <td>-0.000614</td>\n",
       "      <td>-0.004077</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.004247</td>\n",
       "      <td>-0.002622</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.003888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.001835  0.000878  0.000793  0.000247  0.000728  0.001072  0.002347   \n",
       "1  0.003758 -0.002548  0.006330 -0.011134  0.000164  0.005120 -0.002903   \n",
       "2  0.001106  0.001177  0.001245  0.001310  0.001369  0.001383  0.001337   \n",
       "3  0.000363  0.003034  0.003668  0.002877  0.002038  0.000546  0.002638   \n",
       "4  0.003795  0.009750  0.001765  0.005447  0.004424  0.003195  0.001814   \n",
       "\n",
       "       7         8         9     ...      1691      1692      1693      1694  \\\n",
       "0  0.000115  0.000966  0.001123  ... -0.003352 -0.004829 -0.003905 -0.002224   \n",
       "1 -0.003049 -0.006294  0.003996  ... -0.010454  0.004719 -0.009114 -0.001578   \n",
       "2  0.001292  0.001355  0.001509  ... -0.003437 -0.003414 -0.003390 -0.003366   \n",
       "3 -0.002580 -0.000733 -0.003121  ... -0.002536 -0.007373 -0.000276 -0.002173   \n",
       "4 -0.004758  0.005961  0.005176  ... -0.003264  0.001135 -0.008510 -0.000614   \n",
       "\n",
       "       1695      1696      1697      1698      1699      1700  \n",
       "0 -0.002386 -0.003332 -0.003701 -0.003463 -0.004489 -0.005034  \n",
       "1 -0.002636 -0.001219  0.000561 -0.004356  0.002053 -0.007237  \n",
       "2 -0.003341 -0.003316 -0.003289 -0.003263 -0.003236 -0.003209  \n",
       "3 -0.007754 -0.006247 -0.003583 -0.004043 -0.002472 -0.008282  \n",
       "4 -0.004077 -0.013175 -0.004247 -0.002622 -0.000083 -0.003888  \n",
       "\n",
       "[5 rows x 1701 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf=[modsub1,modsub2,modsub3,modsub4,modsub5,modsub6,modsub7,modsub8,modsub9]\n",
    "cent_subs=pd.concat(subdf)\n",
    "cent_subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "909afe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cent_subs['substrate']=ys\n",
    "#cent_subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5e4c570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1691</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003352</td>\n",
       "      <td>-0.004829</td>\n",
       "      <td>-0.003905</td>\n",
       "      <td>-0.002224</td>\n",
       "      <td>-0.002386</td>\n",
       "      <td>-0.003332</td>\n",
       "      <td>-0.003701</td>\n",
       "      <td>-0.003463</td>\n",
       "      <td>-0.004489</td>\n",
       "      <td>-0.005034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003758</td>\n",
       "      <td>-0.002548</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>-0.011134</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.005120</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>-0.003049</td>\n",
       "      <td>-0.006294</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010454</td>\n",
       "      <td>0.004719</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>-0.001578</td>\n",
       "      <td>-0.002636</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>-0.004356</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>-0.007237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003437</td>\n",
       "      <td>-0.003414</td>\n",
       "      <td>-0.003390</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>-0.003341</td>\n",
       "      <td>-0.003316</td>\n",
       "      <td>-0.003289</td>\n",
       "      <td>-0.003263</td>\n",
       "      <td>-0.003236</td>\n",
       "      <td>-0.003209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>-0.002580</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>-0.003121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002536</td>\n",
       "      <td>-0.007373</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-0.002173</td>\n",
       "      <td>-0.007754</td>\n",
       "      <td>-0.006247</td>\n",
       "      <td>-0.003583</td>\n",
       "      <td>-0.004043</td>\n",
       "      <td>-0.002472</td>\n",
       "      <td>-0.008282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.009750</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>-0.004758</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003264</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>-0.008510</td>\n",
       "      <td>-0.000614</td>\n",
       "      <td>-0.004077</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.004247</td>\n",
       "      <td>-0.002622</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.003888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.001835  0.000878  0.000793  0.000247  0.000728  0.001072  0.002347   \n",
       "1  0.003758 -0.002548  0.006330 -0.011134  0.000164  0.005120 -0.002903   \n",
       "2  0.001106  0.001177  0.001245  0.001310  0.001369  0.001383  0.001337   \n",
       "3  0.000363  0.003034  0.003668  0.002877  0.002038  0.000546  0.002638   \n",
       "4  0.003795  0.009750  0.001765  0.005447  0.004424  0.003195  0.001814   \n",
       "\n",
       "       7         8         9     ...      1691      1692      1693      1694  \\\n",
       "0  0.000115  0.000966  0.001123  ... -0.003352 -0.004829 -0.003905 -0.002224   \n",
       "1 -0.003049 -0.006294  0.003996  ... -0.010454  0.004719 -0.009114 -0.001578   \n",
       "2  0.001292  0.001355  0.001509  ... -0.003437 -0.003414 -0.003390 -0.003366   \n",
       "3 -0.002580 -0.000733 -0.003121  ... -0.002536 -0.007373 -0.000276 -0.002173   \n",
       "4 -0.004758  0.005961  0.005176  ... -0.003264  0.001135 -0.008510 -0.000614   \n",
       "\n",
       "       1695      1696      1697      1698      1699      1700  \n",
       "0 -0.002386 -0.003332 -0.003701 -0.003463 -0.004489 -0.005034  \n",
       "1 -0.002636 -0.001219  0.000561 -0.004356  0.002053 -0.007237  \n",
       "2 -0.003341 -0.003316 -0.003289 -0.003263 -0.003236 -0.003209  \n",
       "3 -0.007754 -0.006247 -0.003583 -0.004043 -0.002472 -0.008282  \n",
       "4 -0.004077 -0.013175 -0.004247 -0.002622 -0.000083 -0.003888  \n",
       "\n",
       "[5 rows x 1701 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "b=cent_subs.iloc[:,0:1701]\n",
    "b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0251c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1691</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009607</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.005611</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017547</td>\n",
       "      <td>-0.025276</td>\n",
       "      <td>-0.020442</td>\n",
       "      <td>-0.011642</td>\n",
       "      <td>-0.012491</td>\n",
       "      <td>-0.017440</td>\n",
       "      <td>-0.019376</td>\n",
       "      <td>-0.018127</td>\n",
       "      <td>-0.023498</td>\n",
       "      <td>-0.026352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013683</td>\n",
       "      <td>-0.009277</td>\n",
       "      <td>0.023046</td>\n",
       "      <td>-0.040534</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.018640</td>\n",
       "      <td>-0.010568</td>\n",
       "      <td>-0.011098</td>\n",
       "      <td>-0.022913</td>\n",
       "      <td>0.014546</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038060</td>\n",
       "      <td>0.017181</td>\n",
       "      <td>-0.033179</td>\n",
       "      <td>-0.005745</td>\n",
       "      <td>-0.009596</td>\n",
       "      <td>-0.004439</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>-0.015857</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>-0.026345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005920</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>0.006659</td>\n",
       "      <td>0.007008</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018391</td>\n",
       "      <td>-0.018267</td>\n",
       "      <td>-0.018138</td>\n",
       "      <td>-0.018009</td>\n",
       "      <td>-0.017876</td>\n",
       "      <td>-0.017742</td>\n",
       "      <td>-0.017598</td>\n",
       "      <td>-0.017460</td>\n",
       "      <td>-0.017316</td>\n",
       "      <td>-0.017168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.014365</td>\n",
       "      <td>0.017369</td>\n",
       "      <td>0.013624</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>0.012490</td>\n",
       "      <td>-0.012215</td>\n",
       "      <td>-0.003470</td>\n",
       "      <td>-0.014777</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012008</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>-0.001307</td>\n",
       "      <td>-0.010292</td>\n",
       "      <td>-0.036719</td>\n",
       "      <td>-0.029583</td>\n",
       "      <td>-0.016964</td>\n",
       "      <td>-0.019143</td>\n",
       "      <td>-0.011705</td>\n",
       "      <td>-0.039219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014017</td>\n",
       "      <td>0.036012</td>\n",
       "      <td>0.006521</td>\n",
       "      <td>0.020120</td>\n",
       "      <td>0.016342</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.006702</td>\n",
       "      <td>-0.017573</td>\n",
       "      <td>0.022017</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012055</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>-0.031434</td>\n",
       "      <td>-0.002269</td>\n",
       "      <td>-0.015060</td>\n",
       "      <td>-0.048664</td>\n",
       "      <td>-0.015689</td>\n",
       "      <td>-0.009685</td>\n",
       "      <td>-0.000306</td>\n",
       "      <td>-0.014363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.009607  0.004595  0.004153  0.001293  0.003809  0.005611  0.012284   \n",
       "1  0.013683 -0.009277  0.023046 -0.040534  0.000598  0.018640 -0.010568   \n",
       "2  0.005920  0.006296  0.006659  0.007008  0.007325  0.007399  0.007152   \n",
       "3  0.001720  0.014365  0.017369  0.013624  0.009649  0.002587  0.012490   \n",
       "4  0.014017  0.036012  0.006521  0.020120  0.016342  0.011801  0.006702   \n",
       "\n",
       "       7         8         9     ...      1691      1692      1693      1694  \\\n",
       "0  0.000603  0.005059  0.005881  ... -0.017547 -0.025276 -0.020442 -0.011642   \n",
       "1 -0.011098 -0.022913  0.014546  ... -0.038060  0.017181 -0.033179 -0.005745   \n",
       "2  0.006912  0.007250  0.008074  ... -0.018391 -0.018267 -0.018138 -0.018009   \n",
       "3 -0.012215 -0.003470 -0.014777  ... -0.012008 -0.034912 -0.001307 -0.010292   \n",
       "4 -0.017573  0.022017  0.019119  ... -0.012055  0.004192 -0.031434 -0.002269   \n",
       "\n",
       "       1695      1696      1697      1698      1699      1700  \n",
       "0 -0.012491 -0.017440 -0.019376 -0.018127 -0.023498 -0.026352  \n",
       "1 -0.009596 -0.004439  0.002042 -0.015857  0.007474 -0.026345  \n",
       "2 -0.017876 -0.017742 -0.017598 -0.017460 -0.017316 -0.017168  \n",
       "3 -0.036719 -0.029583 -0.016964 -0.019143 -0.011705 -0.039219  \n",
       "4 -0.015060 -0.048664 -0.015689 -0.009685 -0.000306 -0.014363  \n",
       "\n",
       "[5 rows x 1701 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize(b)\n",
    "b1=b.apply(lambda x: x/(x**2).sum()**.5, axis=1)\n",
    "b1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "588a69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b1['substrate']=ys\n",
    "#b1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c49d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 28 29 31 32 33 34 35 36 37 38 39 41 42 43 44 45 46 47 48 49 50 51\n",
      " 52 53 54]\n"
     ]
    }
   ],
   "source": [
    "a1=np.array(dfsub1['Analyte'].values.tolist())\n",
    "a2=np.array(dfsub2['Analyte'].values.tolist())\n",
    "a3=np.array(dfsub3['Analyte'].values.tolist())\n",
    "a4=np.array(dfsub4['Analyte'].values.tolist())\n",
    "a5=np.array(dfsub5['Analyte'].values.tolist())\n",
    "a6=np.array(dfsub6['Analyte'].values.tolist())\n",
    "a7=np.array(dfsub7['Analyte'].values.tolist())\n",
    "a8=np.array(dfsub8['Analyte'].values.tolist())\n",
    "a9=np.array(dfsub9['Analyte'].values.tolist())\n",
    "y=np.concatenate((a1,a2,a3,a4,a5,a6,a7,a8,a9),axis=None)\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07350d0c",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4382320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import tempfile\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, roc_auc_score\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85ec7dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dimension:\n",
      "(36720, 1701)\n",
      "Test dimension:\n",
      "(9180, 55)\n"
     ]
    }
   ],
   "source": [
    "# Configuration options\n",
    "feature_vector_length = 1701\n",
    "num_classes = 55\n",
    "X=b1.iloc[:,0:1701]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size=0.2,random_state=0)\n",
    "# Convert target classes to categorical ones\n",
    "ytrain=Y_train-1\n",
    "ytest=Y_test-1\n",
    "Y_train = to_categorical(ytrain, num_classes)\n",
    "Y_test = to_categorical(ytest, num_classes)\n",
    "print('Train dimension:')\n",
    "print(X_train.shape)\n",
    "print('Test dimension:')\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "651b8b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (1701,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 1701)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               510600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 55)                11055     \n",
      "=================================================================\n",
      "Total params: 581,855\n",
      "Trainable params: 581,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set the input shape\n",
    "input_shape = (feature_vector_length,)\n",
    "print(f'Feature shape: {input_shape}')\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.2, input_shape=input_shape))\n",
    "model.add(Dense(300, input_shape=input_shape, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdf25ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "196/196 [==============================] - 6s 28ms/step - loss: 2.3756 - accuracy: 0.4639 - val_loss: 1.0293 - val_accuracy: 0.7676\n",
      "Epoch 2/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.8763 - accuracy: 0.7941 - val_loss: 0.7932 - val_accuracy: 0.8109\n",
      "Epoch 3/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.6745 - accuracy: 0.8348 - val_loss: 0.7366 - val_accuracy: 0.8212\n",
      "Epoch 4/50\n",
      "196/196 [==============================] - 6s 28ms/step - loss: 0.5584 - accuracy: 0.8590 - val_loss: 0.7155 - val_accuracy: 0.8201\n",
      "Epoch 5/50\n",
      "196/196 [==============================] - 6s 29ms/step - loss: 0.4666 - accuracy: 0.8791 - val_loss: 0.7165 - val_accuracy: 0.8208\n",
      "Epoch 6/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.3980 - accuracy: 0.8951 - val_loss: 0.7206 - val_accuracy: 0.8233\n",
      "Epoch 7/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.3346 - accuracy: 0.9122 - val_loss: 0.7397 - val_accuracy: 0.8227\n",
      "Epoch 8/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.2771 - accuracy: 0.9256 - val_loss: 0.7614 - val_accuracy: 0.8197\n",
      "Epoch 9/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.2274 - accuracy: 0.9398 - val_loss: 0.7820 - val_accuracy: 0.8194\n",
      "Epoch 10/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.1918 - accuracy: 0.9495 - val_loss: 0.8056 - val_accuracy: 0.8178\n",
      "Epoch 11/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.1533 - accuracy: 0.9610 - val_loss: 0.8298 - val_accuracy: 0.8167\n",
      "Epoch 12/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.1315 - accuracy: 0.9661 - val_loss: 0.8509 - val_accuracy: 0.8160\n",
      "Epoch 13/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.1057 - accuracy: 0.9747 - val_loss: 0.8752 - val_accuracy: 0.8136\n",
      "Epoch 14/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.0932 - accuracy: 0.9766 - val_loss: 0.8911 - val_accuracy: 0.8133\n",
      "Epoch 15/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0801 - accuracy: 0.9809 - val_loss: 0.9063 - val_accuracy: 0.8151\n",
      "Epoch 16/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0714 - accuracy: 0.9831 - val_loss: 0.9203 - val_accuracy: 0.8141\n",
      "Epoch 17/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0624 - accuracy: 0.9856 - val_loss: 0.9311 - val_accuracy: 0.8144\n",
      "Epoch 18/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0574 - accuracy: 0.9856 - val_loss: 0.9437 - val_accuracy: 0.8147\n",
      "Epoch 19/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.0514 - accuracy: 0.9873 - val_loss: 0.9605 - val_accuracy: 0.8144\n",
      "Epoch 20/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.0474 - accuracy: 0.9880 - val_loss: 0.9815 - val_accuracy: 0.8098\n",
      "Epoch 21/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0396 - accuracy: 0.9905 - val_loss: 0.9884 - val_accuracy: 0.8100\n",
      "Epoch 22/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0359 - accuracy: 0.9909 - val_loss: 1.0076 - val_accuracy: 0.8111\n",
      "Epoch 23/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0357 - accuracy: 0.9914 - val_loss: 1.0068 - val_accuracy: 0.8137\n",
      "Epoch 24/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0325 - accuracy: 0.9921 - val_loss: 1.0176 - val_accuracy: 0.8124\n",
      "Epoch 25/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.0355 - accuracy: 0.9906 - val_loss: 1.0259 - val_accuracy: 0.8130\n",
      "Epoch 26/50\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 0.0314 - accuracy: 0.9921 - val_loss: 1.0291 - val_accuracy: 0.8133\n",
      "Epoch 27/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0277 - accuracy: 0.9928 - val_loss: 1.0385 - val_accuracy: 0.8148\n",
      "Epoch 28/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0290 - accuracy: 0.9924 - val_loss: 1.0522 - val_accuracy: 0.8140\n",
      "Epoch 29/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0259 - accuracy: 0.9930 - val_loss: 1.0533 - val_accuracy: 0.8109\n",
      "Epoch 30/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0267 - accuracy: 0.9923 - val_loss: 1.0692 - val_accuracy: 0.8118\n",
      "Epoch 31/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.0231 - accuracy: 0.9942 - val_loss: 1.0638 - val_accuracy: 0.8150\n",
      "Epoch 32/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.0227 - accuracy: 0.9937 - val_loss: 1.0692 - val_accuracy: 0.8158\n",
      "Epoch 33/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.0230 - accuracy: 0.9938 - val_loss: 1.0737 - val_accuracy: 0.8136\n",
      "Epoch 34/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.0209 - accuracy: 0.9944 - val_loss: 1.0856 - val_accuracy: 0.8140\n",
      "Epoch 35/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0227 - accuracy: 0.9938 - val_loss: 1.0801 - val_accuracy: 0.8144\n",
      "Epoch 36/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0206 - accuracy: 0.9947 - val_loss: 1.0794 - val_accuracy: 0.8155\n",
      "Epoch 37/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0182 - accuracy: 0.9953 - val_loss: 1.0994 - val_accuracy: 0.8133\n",
      "Epoch 38/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0191 - accuracy: 0.9944 - val_loss: 1.0927 - val_accuracy: 0.8151\n",
      "Epoch 39/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0175 - accuracy: 0.9953 - val_loss: 1.1158 - val_accuracy: 0.8125\n",
      "Epoch 40/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0204 - accuracy: 0.9947 - val_loss: 1.1145 - val_accuracy: 0.8121\n",
      "Epoch 41/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.0152 - accuracy: 0.9962 - val_loss: 1.1207 - val_accuracy: 0.8141\n",
      "Epoch 42/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0175 - accuracy: 0.9950 - val_loss: 1.1265 - val_accuracy: 0.8140\n",
      "Epoch 43/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0163 - accuracy: 0.9949 - val_loss: 1.1442 - val_accuracy: 0.8129\n",
      "Epoch 44/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.0175 - accuracy: 0.9948 - val_loss: 1.1460 - val_accuracy: 0.8148\n",
      "Epoch 45/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0144 - accuracy: 0.9959 - val_loss: 1.1660 - val_accuracy: 0.8145\n",
      "Epoch 46/50\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 0.0188 - accuracy: 0.9940 - val_loss: 1.1702 - val_accuracy: 0.8110\n",
      "Epoch 47/50\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 0.0149 - accuracy: 0.9956 - val_loss: 1.1705 - val_accuracy: 0.8158\n",
      "Epoch 48/50\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 0.0144 - accuracy: 0.9959 - val_loss: 1.1851 - val_accuracy: 0.8110\n",
      "Epoch 49/50\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 1.1888 - val_accuracy: 0.8154\n",
      "Epoch 50/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0159 - accuracy: 0.9951 - val_loss: 1.1802 - val_accuracy: 0.8133\n"
     ]
    }
   ],
   "source": [
    "# Configure the model and start training\n",
    "import time\n",
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "baseline_history=model.fit(X_train, Y_train, epochs=50, batch_size=150, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0fe9ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:---  258.07695603370667 seconds ---\n",
      "287/287 [==============================] - 1s 4ms/step - loss: 1.1499 - accuracy: 0.8180\n",
      "[1.1498568058013916, 0.8179738521575928]\n",
      "Test results - Loss: 1.1498568058013916 - Accuracy: 0.8179738521575928%\n",
      "--- 1.9938347339630127 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"Training time:---  %s seconds ---\" % (time.time() - start_time))\n",
    "#Test the model after training\n",
    "start_time=time.time()\n",
    "test_results = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(test_results)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4405f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for plotting\n",
    "mpl.rcParams['figure.figsize'] = (12, 8)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "def plot_metrics(history):\n",
    "  metrics =  ['loss', 'accuracy']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(1,2,n+1)\n",
    "    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39173d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(baseline_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d096f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-7b108664374f>:6: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "f1_scores in training set 0.9626806558039379 f1_scores in testing set 0.8182398166405525\n"
     ]
    }
   ],
   "source": [
    "#Computing F1-score\n",
    "train_features = np.array(X_train)\n",
    "test_features = np.array(X_test)\n",
    "train_labels=np.array(Y_train)\n",
    "test_labels=np.array(Y_test)\n",
    "train_predictions_baseline = model.predict_classes(train_features, batch_size=150)\n",
    "f1_train=sklearn.metrics.f1_score(ytrain, train_predictions_baseline, average=\"weighted\")\n",
    "test_predictions_baseline = model.predict_classes(test_features, batch_size=150)\n",
    "f1_test=sklearn.metrics.f1_score(ytest, test_predictions_baseline, average=\"weighted\")\n",
    "print('f1_scores in training set',f1_train,'f1_scores in testing set',f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d35efca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION ON TESTING DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87       174\n",
      "           1       0.78      0.76      0.77       173\n",
      "           2       0.84      0.82      0.83       178\n",
      "           3       0.79      0.75      0.77       174\n",
      "           4       0.85      0.87      0.86       198\n",
      "           5       0.88      0.86      0.87       196\n",
      "           6       0.68      0.86      0.76       170\n",
      "           7       0.79      0.87      0.83       188\n",
      "           8       0.89      0.84      0.87       200\n",
      "           9       0.78      0.81      0.79       166\n",
      "          10       0.93      0.85      0.89       183\n",
      "          11       0.85      0.88      0.86       176\n",
      "          12       0.91      0.71      0.79       164\n",
      "          13       0.85      0.80      0.82       181\n",
      "          14       0.81      0.77      0.79       173\n",
      "          15       0.76      0.82      0.79       160\n",
      "          16       0.82      0.88      0.85       185\n",
      "          17       0.80      0.79      0.79       182\n",
      "          18       0.72      0.82      0.77       170\n",
      "          19       0.84      0.84      0.84       175\n",
      "          20       0.91      0.85      0.88       184\n",
      "          21       0.81      0.80      0.80       183\n",
      "          22       0.82      0.80      0.81       168\n",
      "          23       0.83      0.83      0.83       171\n",
      "          24       0.78      0.80      0.79       184\n",
      "          25       0.75      0.84      0.79       195\n",
      "          27       0.82      0.76      0.79       189\n",
      "          28       0.77      0.70      0.74       178\n",
      "          30       0.75      0.78      0.76       195\n",
      "          31       0.87      0.85      0.86       198\n",
      "          32       0.81      0.71      0.76       186\n",
      "          33       0.77      0.82      0.80       171\n",
      "          34       0.88      0.78      0.83       199\n",
      "          35       0.85      0.86      0.85       160\n",
      "          36       0.86      0.80      0.83       174\n",
      "          37       0.85      0.82      0.84       176\n",
      "          38       0.85      0.79      0.82       170\n",
      "          40       0.85      0.89      0.87       184\n",
      "          41       0.75      0.76      0.76       187\n",
      "          42       0.91      0.83      0.87       172\n",
      "          43       0.78      0.85      0.81       186\n",
      "          44       0.92      0.90      0.91       199\n",
      "          45       0.84      0.73      0.78       177\n",
      "          46       0.78      0.77      0.78       173\n",
      "          47       0.78      0.87      0.82       183\n",
      "          48       0.77      0.84      0.81       177\n",
      "          49       0.79      0.84      0.81       171\n",
      "          50       0.89      0.88      0.89       182\n",
      "          51       0.80      0.84      0.82       176\n",
      "          52       0.71      0.78      0.74       165\n",
      "          53       0.85      0.82      0.83       201\n",
      "\n",
      "    accuracy                           0.82      9180\n",
      "   macro avg       0.82      0.82      0.82      9180\n",
      "weighted avg       0.82      0.82      0.82      9180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics for each class\n",
    "print(\"EVALUATION ON TESTING DATA\")\n",
    "print(classification_report(ytest, test_predictions_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f18beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "dfset3=b1.copy()\n",
    "pca = PCA(n_components=100)\n",
    "pca_result = pca.fit_transform(dfset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c53e3960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dimension:\n",
      "(36720, 100)\n",
      "Test dimension:\n",
      "(9180, 100)\n"
     ]
    }
   ],
   "source": [
    "# Configuration options\n",
    "feature_vector_length = 100\n",
    "num_classes = 55\n",
    "X=pca_result\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size=0.2,random_state=0)\n",
    "# Convert target classes to categorical ones\n",
    "Y_train=Y_train-1\n",
    "Y_test=Y_test-1\n",
    "Y_train = to_categorical(Y_train, num_classes)\n",
    "Y_test = to_categorical(Y_test, num_classes)\n",
    "print('Train dimension:')\n",
    "print(X_train.shape)\n",
    "print('Test dimension:')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8a51fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (100,)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 55)                1155      \n",
      "=================================================================\n",
      "Total params: 3,595\n",
      "Trainable params: 3,595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set the input shape\n",
    "input_shape = (feature_vector_length,)\n",
    "print(f'Feature shape: {input_shape}')\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "#model.add(Dropout(0.2, input_shape=input_shape))\n",
    "model.add(Dense(20, input_shape=input_shape, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65495278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 3.6835 - accuracy: 0.1245 - val_loss: 3.0180 - val_accuracy: 0.3214\n",
      "Epoch 2/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 2.3627 - accuracy: 0.5047 - val_loss: 1.8888 - val_accuracy: 0.6183\n",
      "Epoch 3/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 1.6210 - accuracy: 0.6675 - val_loss: 1.4337 - val_accuracy: 0.6989\n",
      "Epoch 4/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 1.3227 - accuracy: 0.7126 - val_loss: 1.2435 - val_accuracy: 0.7289\n",
      "Epoch 5/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 1.1869 - accuracy: 0.7343 - val_loss: 1.1476 - val_accuracy: 0.7439\n",
      "Epoch 6/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 1.1094 - accuracy: 0.7466 - val_loss: 1.0853 - val_accuracy: 0.7546\n",
      "Epoch 7/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 1.0563 - accuracy: 0.7558 - val_loss: 1.0394 - val_accuracy: 0.7655\n",
      "Epoch 8/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 1.0167 - accuracy: 0.7653 - val_loss: 1.0075 - val_accuracy: 0.7710\n",
      "Epoch 9/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.9845 - accuracy: 0.7704 - val_loss: 0.9787 - val_accuracy: 0.7787\n",
      "Epoch 10/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.9576 - accuracy: 0.7768 - val_loss: 0.9592 - val_accuracy: 0.7774\n",
      "Epoch 11/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.9330 - accuracy: 0.7806 - val_loss: 0.9381 - val_accuracy: 0.7831\n",
      "Epoch 12/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.9137 - accuracy: 0.7848 - val_loss: 0.9173 - val_accuracy: 0.7864\n",
      "Epoch 13/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.8952 - accuracy: 0.7892 - val_loss: 0.9018 - val_accuracy: 0.7883\n",
      "Epoch 14/100\n",
      "459/459 [==============================] - 3s 5ms/step - loss: 0.8791 - accuracy: 0.7915 - val_loss: 0.8867 - val_accuracy: 0.7926\n",
      "Epoch 15/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.8634 - accuracy: 0.7951 - val_loss: 0.8784 - val_accuracy: 0.7926\n",
      "Epoch 16/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.8497 - accuracy: 0.7962 - val_loss: 0.8647 - val_accuracy: 0.7936\n",
      "Epoch 17/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.8368 - accuracy: 0.7993 - val_loss: 0.8563 - val_accuracy: 0.7936\n",
      "Epoch 18/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.8246 - accuracy: 0.8011 - val_loss: 0.8446 - val_accuracy: 0.8002\n",
      "Epoch 19/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.8144 - accuracy: 0.8038 - val_loss: 0.8351 - val_accuracy: 0.7974\n",
      "Epoch 20/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.8043 - accuracy: 0.8053 - val_loss: 0.8282 - val_accuracy: 0.8002\n",
      "Epoch 21/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7949 - accuracy: 0.8061 - val_loss: 0.8206 - val_accuracy: 0.8008\n",
      "Epoch 22/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7862 - accuracy: 0.8065 - val_loss: 0.8141 - val_accuracy: 0.8015\n",
      "Epoch 23/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.7784 - accuracy: 0.8096 - val_loss: 0.8088 - val_accuracy: 0.8039\n",
      "Epoch 24/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.7716 - accuracy: 0.8093 - val_loss: 0.8039 - val_accuracy: 0.8039\n",
      "Epoch 25/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7643 - accuracy: 0.8104 - val_loss: 0.7987 - val_accuracy: 0.8031\n",
      "Epoch 26/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7579 - accuracy: 0.8109 - val_loss: 0.7948 - val_accuracy: 0.8072\n",
      "Epoch 27/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7519 - accuracy: 0.8139 - val_loss: 0.7923 - val_accuracy: 0.8060\n",
      "Epoch 28/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7469 - accuracy: 0.8136 - val_loss: 0.7873 - val_accuracy: 0.8046\n",
      "Epoch 29/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7415 - accuracy: 0.8147 - val_loss: 0.7846 - val_accuracy: 0.8043\n",
      "Epoch 30/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.7371 - accuracy: 0.8153 - val_loss: 0.7811 - val_accuracy: 0.8064\n",
      "Epoch 31/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7319 - accuracy: 0.8172 - val_loss: 0.7803 - val_accuracy: 0.8073\n",
      "Epoch 32/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.7275 - accuracy: 0.8170 - val_loss: 0.7784 - val_accuracy: 0.8071\n",
      "Epoch 33/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7234 - accuracy: 0.8191 - val_loss: 0.7727 - val_accuracy: 0.8066\n",
      "Epoch 34/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7198 - accuracy: 0.8188 - val_loss: 0.7727 - val_accuracy: 0.8064\n",
      "Epoch 35/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7159 - accuracy: 0.8194 - val_loss: 0.7686 - val_accuracy: 0.8088\n",
      "Epoch 36/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7123 - accuracy: 0.8205 - val_loss: 0.7677 - val_accuracy: 0.8069\n",
      "Epoch 37/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7088 - accuracy: 0.8216 - val_loss: 0.7657 - val_accuracy: 0.8084\n",
      "Epoch 38/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7058 - accuracy: 0.8215 - val_loss: 0.7658 - val_accuracy: 0.8075\n",
      "Epoch 39/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.7028 - accuracy: 0.8228 - val_loss: 0.7633 - val_accuracy: 0.8099\n",
      "Epoch 40/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6998 - accuracy: 0.8224 - val_loss: 0.7616 - val_accuracy: 0.8130\n",
      "Epoch 41/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6972 - accuracy: 0.8234 - val_loss: 0.7587 - val_accuracy: 0.8084\n",
      "Epoch 42/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.8245 - val_loss: 0.7580 - val_accuracy: 0.8109\n",
      "Epoch 43/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.8242 - val_loss: 0.7550 - val_accuracy: 0.8105\n",
      "Epoch 44/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.8248 - val_loss: 0.7549 - val_accuracy: 0.8115\n",
      "Epoch 45/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.8253 - val_loss: 0.7549 - val_accuracy: 0.8109\n",
      "Epoch 46/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.8257 - val_loss: 0.7534 - val_accuracy: 0.8095\n",
      "Epoch 47/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6818 - accuracy: 0.8264 - val_loss: 0.7529 - val_accuracy: 0.8105\n",
      "Epoch 48/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6798 - accuracy: 0.8261 - val_loss: 0.7533 - val_accuracy: 0.8083\n",
      "Epoch 49/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6771 - accuracy: 0.8271 - val_loss: 0.7510 - val_accuracy: 0.8115\n",
      "Epoch 50/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6754 - accuracy: 0.8280 - val_loss: 0.7474 - val_accuracy: 0.8121\n",
      "Epoch 51/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6733 - accuracy: 0.8282 - val_loss: 0.7509 - val_accuracy: 0.8110\n",
      "Epoch 52/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6717 - accuracy: 0.8291 - val_loss: 0.7501 - val_accuracy: 0.8107\n",
      "Epoch 53/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6700 - accuracy: 0.8286 - val_loss: 0.7488 - val_accuracy: 0.8125\n",
      "Epoch 54/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6682 - accuracy: 0.8284 - val_loss: 0.7455 - val_accuracy: 0.8111\n",
      "Epoch 55/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6660 - accuracy: 0.8295 - val_loss: 0.7460 - val_accuracy: 0.8110\n",
      "Epoch 56/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6645 - accuracy: 0.8294 - val_loss: 0.7450 - val_accuracy: 0.8130\n",
      "Epoch 57/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6629 - accuracy: 0.8298 - val_loss: 0.7448 - val_accuracy: 0.8117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6609 - accuracy: 0.8304 - val_loss: 0.7445 - val_accuracy: 0.8141\n",
      "Epoch 59/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6596 - accuracy: 0.8313 - val_loss: 0.7432 - val_accuracy: 0.8129\n",
      "Epoch 60/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6581 - accuracy: 0.8308 - val_loss: 0.7414 - val_accuracy: 0.8141\n",
      "Epoch 61/100\n",
      "459/459 [==============================] - 3s 6ms/step - loss: 0.6563 - accuracy: 0.8327 - val_loss: 0.7427 - val_accuracy: 0.8128\n",
      "Epoch 62/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6544 - accuracy: 0.8314 - val_loss: 0.7411 - val_accuracy: 0.8135\n",
      "Epoch 63/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6539 - accuracy: 0.8319 - val_loss: 0.7416 - val_accuracy: 0.8143\n",
      "Epoch 64/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6519 - accuracy: 0.8328 - val_loss: 0.7402 - val_accuracy: 0.8136\n",
      "Epoch 65/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6514 - accuracy: 0.8330 - val_loss: 0.7402 - val_accuracy: 0.8126\n",
      "Epoch 66/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6496 - accuracy: 0.8322 - val_loss: 0.7390 - val_accuracy: 0.8137\n",
      "Epoch 67/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6485 - accuracy: 0.8329 - val_loss: 0.7385 - val_accuracy: 0.8140\n",
      "Epoch 68/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6470 - accuracy: 0.8329 - val_loss: 0.7393 - val_accuracy: 0.8135\n",
      "Epoch 69/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6456 - accuracy: 0.8342 - val_loss: 0.7408 - val_accuracy: 0.8136\n",
      "Epoch 70/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6445 - accuracy: 0.8336 - val_loss: 0.7388 - val_accuracy: 0.8144\n",
      "Epoch 71/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6431 - accuracy: 0.8348 - val_loss: 0.7377 - val_accuracy: 0.8144\n",
      "Epoch 72/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6421 - accuracy: 0.8350 - val_loss: 0.7384 - val_accuracy: 0.8154\n",
      "Epoch 73/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6409 - accuracy: 0.8353 - val_loss: 0.7380 - val_accuracy: 0.8117\n",
      "Epoch 74/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6396 - accuracy: 0.8348 - val_loss: 0.7392 - val_accuracy: 0.8156\n",
      "Epoch 75/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6390 - accuracy: 0.8359 - val_loss: 0.7372 - val_accuracy: 0.8171\n",
      "Epoch 76/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6376 - accuracy: 0.8352 - val_loss: 0.7377 - val_accuracy: 0.8139\n",
      "Epoch 77/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6365 - accuracy: 0.8360 - val_loss: 0.7367 - val_accuracy: 0.8156\n",
      "Epoch 78/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6352 - accuracy: 0.8356 - val_loss: 0.7368 - val_accuracy: 0.8160\n",
      "Epoch 79/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6340 - accuracy: 0.8361 - val_loss: 0.7368 - val_accuracy: 0.8144\n",
      "Epoch 80/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6337 - accuracy: 0.8363 - val_loss: 0.7341 - val_accuracy: 0.8160\n",
      "Epoch 81/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6323 - accuracy: 0.8363 - val_loss: 0.7362 - val_accuracy: 0.8156\n",
      "Epoch 82/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6309 - accuracy: 0.8377 - val_loss: 0.7361 - val_accuracy: 0.8170\n",
      "Epoch 83/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6298 - accuracy: 0.8374 - val_loss: 0.7351 - val_accuracy: 0.8159\n",
      "Epoch 84/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6297 - accuracy: 0.8371 - val_loss: 0.7346 - val_accuracy: 0.8164\n",
      "Epoch 85/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6281 - accuracy: 0.8382 - val_loss: 0.7350 - val_accuracy: 0.8152\n",
      "Epoch 86/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6276 - accuracy: 0.8378 - val_loss: 0.7328 - val_accuracy: 0.8185\n",
      "Epoch 87/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6264 - accuracy: 0.8375 - val_loss: 0.7371 - val_accuracy: 0.8169\n",
      "Epoch 88/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6256 - accuracy: 0.8381 - val_loss: 0.7324 - val_accuracy: 0.8177\n",
      "Epoch 89/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6247 - accuracy: 0.8387 - val_loss: 0.7332 - val_accuracy: 0.8155\n",
      "Epoch 90/100\n",
      "459/459 [==============================] - 2s 3ms/step - loss: 0.6236 - accuracy: 0.8385 - val_loss: 0.7314 - val_accuracy: 0.8174\n",
      "Epoch 91/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6232 - accuracy: 0.8377 - val_loss: 0.7334 - val_accuracy: 0.8164\n",
      "Epoch 92/100\n",
      "459/459 [==============================] - 2s 5ms/step - loss: 0.6222 - accuracy: 0.8391 - val_loss: 0.7341 - val_accuracy: 0.8162\n",
      "Epoch 93/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6218 - accuracy: 0.8382 - val_loss: 0.7337 - val_accuracy: 0.8189\n",
      "Epoch 94/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6207 - accuracy: 0.8392 - val_loss: 0.7318 - val_accuracy: 0.8174\n",
      "Epoch 95/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6201 - accuracy: 0.8382 - val_loss: 0.7311 - val_accuracy: 0.8159\n",
      "Epoch 96/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6191 - accuracy: 0.8401 - val_loss: 0.7295 - val_accuracy: 0.8174\n",
      "Epoch 97/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6188 - accuracy: 0.8397 - val_loss: 0.7333 - val_accuracy: 0.8169\n",
      "Epoch 98/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6182 - accuracy: 0.8395 - val_loss: 0.7317 - val_accuracy: 0.8173\n",
      "Epoch 99/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6170 - accuracy: 0.8397 - val_loss: 0.7336 - val_accuracy: 0.8173\n",
      "Epoch 100/100\n",
      "459/459 [==============================] - 2s 4ms/step - loss: 0.6161 - accuracy: 0.8400 - val_loss: 0.7291 - val_accuracy: 0.8190\n",
      "--- 202.62480449676514 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Configure the model and start training\n",
    "import time\n",
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "baseline_history=model.fit(X_train, Y_train, epochs=100, batch_size=64, verbose=1, validation_split=0.2)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d67e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 1s 2ms/step - loss: 0.6926 - accuracy: 0.8202\n",
      "[0.6926056742668152, 0.8201525211334229]\n",
      "Test results - Loss: 0.6926056742668152 - Accuracy: 0.8201525211334229%\n",
      "--- 0.9138672351837158 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Test the model after training\n",
    "start_time=time.time()\n",
    "test_results = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(test_results)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384f75e",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8069d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.datasets import load_iris\n",
    "from numpy import unique\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22238511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45900, 1701, 1)\n"
     ]
    }
   ],
   "source": [
    "x=b1.iloc[:,0:1701].to_numpy()\n",
    "x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "print(x.shape)\n",
    "y=y-1\n",
    "#print(unique(y))\n",
    "#print(unique(y).sum())\n",
    "\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2d802ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 1699, 32)          128       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1699, 64)          2112      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 566, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 566, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 564, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 564, 64)           2112      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 188, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 188, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12032)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 55)                661815    \n",
      "=================================================================\n",
      "Total params: 672,343\n",
      "Trainable params: 672,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "115/115 [==============================] - 134s 1s/step - loss: 2.4701 - accuracy: 0.3955 - val_loss: 0.7710 - val_accuracy: 0.8204\n",
      "Epoch 2/30\n",
      "115/115 [==============================] - 133s 1s/step - loss: 0.8028 - accuracy: 0.8054 - val_loss: 0.6715 - val_accuracy: 0.8427\n",
      "Epoch 3/30\n",
      "115/115 [==============================] - 136s 1s/step - loss: 0.6957 - accuracy: 0.8270 - val_loss: 0.6568 - val_accuracy: 0.8431\n",
      "Epoch 4/30\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.6358 - accuracy: 0.8412 - val_loss: 0.6735 - val_accuracy: 0.8427\n",
      "Epoch 5/30\n",
      "115/115 [==============================] - 133s 1s/step - loss: 0.6081 - accuracy: 0.8465 - val_loss: 0.6721 - val_accuracy: 0.8361\n",
      "Epoch 6/30\n",
      "115/115 [==============================] - 134s 1s/step - loss: 0.5757 - accuracy: 0.8531 - val_loss: 0.6980 - val_accuracy: 0.8352\n",
      "Epoch 7/30\n",
      "115/115 [==============================] - 134s 1s/step - loss: 0.5513 - accuracy: 0.8563 - val_loss: 0.7083 - val_accuracy: 0.8322\n",
      "Epoch 8/30\n",
      "115/115 [==============================] - 133s 1s/step - loss: 0.5346 - accuracy: 0.8612 - val_loss: 0.7014 - val_accuracy: 0.8351\n",
      "Epoch 9/30\n",
      "115/115 [==============================] - 135s 1s/step - loss: 0.5164 - accuracy: 0.8624 - val_loss: 0.7012 - val_accuracy: 0.8291\n",
      "Epoch 10/30\n",
      "115/115 [==============================] - 133s 1s/step - loss: 0.5039 - accuracy: 0.8642 - val_loss: 0.7155 - val_accuracy: 0.8316\n",
      "Epoch 11/30\n",
      "115/115 [==============================] - 133s 1s/step - loss: 0.4847 - accuracy: 0.8700 - val_loss: 0.7361 - val_accuracy: 0.8303\n",
      "Epoch 12/30\n",
      "115/115 [==============================] - 131s 1s/step - loss: 0.4736 - accuracy: 0.8710 - val_loss: 0.7268 - val_accuracy: 0.8286\n",
      "Epoch 13/30\n",
      "115/115 [==============================] - 135s 1s/step - loss: 0.4593 - accuracy: 0.8725 - val_loss: 0.7456 - val_accuracy: 0.8294\n",
      "Epoch 14/30\n",
      "115/115 [==============================] - 133s 1s/step - loss: 0.4442 - accuracy: 0.8779 - val_loss: 0.7404 - val_accuracy: 0.8287\n",
      "Epoch 15/30\n",
      "115/115 [==============================] - 123s 1s/step - loss: 0.4281 - accuracy: 0.8797 - val_loss: 0.7555 - val_accuracy: 0.8271\n",
      "Epoch 16/30\n",
      "115/115 [==============================] - 90s 781ms/step - loss: 0.4168 - accuracy: 0.8832 - val_loss: 0.7608 - val_accuracy: 0.8257\n",
      "Epoch 17/30\n",
      "115/115 [==============================] - 72s 629ms/step - loss: 0.4050 - accuracy: 0.8867 - val_loss: 0.7915 - val_accuracy: 0.8257\n",
      "Epoch 18/30\n",
      "115/115 [==============================] - 72s 627ms/step - loss: 0.3865 - accuracy: 0.8894 - val_loss: 0.7871 - val_accuracy: 0.8279\n",
      "Epoch 19/30\n",
      "115/115 [==============================] - 72s 628ms/step - loss: 0.3663 - accuracy: 0.8930 - val_loss: 0.8170 - val_accuracy: 0.8231\n",
      "Epoch 20/30\n",
      "115/115 [==============================] - 73s 633ms/step - loss: 0.3545 - accuracy: 0.8981 - val_loss: 0.8011 - val_accuracy: 0.8234\n",
      "Epoch 21/30\n",
      "115/115 [==============================] - 72s 629ms/step - loss: 0.3385 - accuracy: 0.9006 - val_loss: 0.8266 - val_accuracy: 0.8230\n",
      "Epoch 22/30\n",
      "115/115 [==============================] - 72s 629ms/step - loss: 0.3230 - accuracy: 0.9035 - val_loss: 0.8415 - val_accuracy: 0.8218\n",
      "Epoch 23/30\n",
      "115/115 [==============================] - 72s 629ms/step - loss: 0.3029 - accuracy: 0.9080 - val_loss: 0.8273 - val_accuracy: 0.8212\n",
      "Epoch 24/30\n",
      "115/115 [==============================] - 75s 649ms/step - loss: 0.2888 - accuracy: 0.9127 - val_loss: 0.8627 - val_accuracy: 0.8211\n",
      "Epoch 25/30\n",
      "115/115 [==============================] - 73s 634ms/step - loss: 0.2754 - accuracy: 0.9139 - val_loss: 0.8921 - val_accuracy: 0.8205\n",
      "Epoch 26/30\n",
      "115/115 [==============================] - 73s 633ms/step - loss: 0.2589 - accuracy: 0.9187 - val_loss: 0.9095 - val_accuracy: 0.8224\n",
      "Epoch 27/30\n",
      "115/115 [==============================] - 73s 638ms/step - loss: 0.2420 - accuracy: 0.9264 - val_loss: 0.9358 - val_accuracy: 0.8209\n",
      "Epoch 28/30\n",
      "115/115 [==============================] - 74s 646ms/step - loss: 0.2254 - accuracy: 0.9290 - val_loss: 0.9263 - val_accuracy: 0.8214\n",
      "Epoch 29/30\n",
      "115/115 [==============================] - 76s 662ms/step - loss: 0.2103 - accuracy: 0.9348 - val_loss: 0.9610 - val_accuracy: 0.8212\n",
      "Epoch 30/30\n",
      "115/115 [==============================] - 74s 643ms/step - loss: 0.2028 - accuracy: 0.9357 - val_loss: 0.9566 - val_accuracy: 0.8190\n",
      "1148/1148 [==============================] - 26s 23ms/step - loss: 0.2298 - accuracy: 0.9599\n",
      "Loss: 0.22976472973823547  Accuracy: 0.9598584175109863\n",
      "--- 3166.2367510795593 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(32, 3, activation=\"relu\", input_shape=(1701,1)))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(32, 3, activation=\"relu\", input_shape=(1701,1)))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(55, activation = 'softmax'))\n",
    "start_time = time.time()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "model.summary()\n",
    "baseline_history=model.fit(xtrain, ytrain, epochs=30, batch_size=256, verbose=1, validation_split=0.2)\n",
    "#model.fit(xtrain, ytrain, batch_size=256,epochs=25,  validation_split=.2, verbose=1)\n",
    "acc = model.evaluate(xtrain, ytrain)\n",
    "print(\"Loss:\", acc[0], \" Accuracy:\", acc[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edccaf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 6s 22ms/step - loss: 1.0323 - accuracy: 0.8119\n",
      "[1.0323060750961304, 0.8118736147880554]\n",
      "Test results - Loss: 1.0323060750961304 - Accuracy: 0.8118736147880554%\n",
      "--- 6.517737150192261 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#Test the model after training\n",
    "start_time=time.time()\n",
    "test_results = model.evaluate(xtest, ytest, verbose=1)\n",
    "print(test_results)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b85bbab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(baseline_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "880db547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_scores in training set 0.9599153571264631 f1_scores in testing set 0.8129856831416836\n"
     ]
    }
   ],
   "source": [
    "#Computing F1-score\n",
    "import sklearn\n",
    "train_features = np.array(xtrain)\n",
    "test_features = np.array(xtest)\n",
    "train_labels=np.array(ytrain)\n",
    "test_labels=np.array(ytest)\n",
    "train_predictions_baseline = model.predict_classes(train_features, batch_size=64)\n",
    "#train_predictions_baseline =np.argmax(model.predict(train_features, batch_size=64)>0.5).astype(\"int32\")\n",
    "f1_train=sklearn.metrics.f1_score(ytrain, train_predictions_baseline, average=\"weighted\")\n",
    "test_predictions_baseline = model.predict_classes(test_features, batch_size=64)\n",
    "f1_test=sklearn.metrics.f1_score(ytest, test_predictions_baseline, average=\"weighted\")\n",
    "print('f1_scores in training set',f1_train,'f1_scores in testing set',f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57ac18ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION ON TESTING DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84       183\n",
      "           1       0.73      0.77      0.75       202\n",
      "           2       0.87      0.85      0.86       191\n",
      "           3       0.83      0.73      0.78       187\n",
      "           4       0.83      0.84      0.83       163\n",
      "           5       0.83      0.86      0.84       177\n",
      "           6       0.80      0.89      0.84       176\n",
      "           7       0.81      0.81      0.81       190\n",
      "           8       0.82      0.86      0.84       190\n",
      "           9       0.86      0.79      0.83       190\n",
      "          10       0.85      0.85      0.85       164\n",
      "          11       0.77      0.86      0.81       189\n",
      "          12       0.91      0.79      0.84       197\n",
      "          13       0.83      0.80      0.81       193\n",
      "          14       0.79      0.81      0.80       171\n",
      "          15       0.74      0.80      0.77       172\n",
      "          16       0.90      0.81      0.85       183\n",
      "          17       0.84      0.79      0.81       185\n",
      "          18       0.87      0.77      0.82       177\n",
      "          19       0.79      0.83      0.81       175\n",
      "          20       0.86      0.86      0.86       162\n",
      "          21       0.82      0.86      0.84       157\n",
      "          22       0.75      0.79      0.77       163\n",
      "          23       0.78      0.86      0.81       153\n",
      "          24       0.73      0.82      0.77       165\n",
      "          25       0.78      0.77      0.77       163\n",
      "          27       0.80      0.80      0.80       175\n",
      "          28       0.83      0.79      0.81       189\n",
      "          30       0.74      0.81      0.78       167\n",
      "          31       0.88      0.83      0.85       196\n",
      "          32       0.67      0.78      0.72       175\n",
      "          33       0.74      0.83      0.78       183\n",
      "          34       0.90      0.79      0.84       196\n",
      "          35       0.91      0.86      0.89       181\n",
      "          36       0.86      0.79      0.82       188\n",
      "          37       0.80      0.83      0.82       185\n",
      "          38       0.81      0.72      0.76       174\n",
      "          40       0.89      0.83      0.86       195\n",
      "          41       0.61      0.81      0.70       155\n",
      "          42       0.84      0.82      0.83       176\n",
      "          43       0.87      0.78      0.82       173\n",
      "          44       0.87      0.87      0.87       177\n",
      "          45       0.70      0.79      0.75       194\n",
      "          46       0.89      0.69      0.78       194\n",
      "          47       0.69      0.83      0.75       172\n",
      "          48       0.96      0.82      0.88       182\n",
      "          49       0.82      0.79      0.81       184\n",
      "          50       0.91      0.88      0.90       195\n",
      "          51       0.91      0.82      0.86       174\n",
      "          52       0.75      0.80      0.77       172\n",
      "          53       0.76      0.79      0.78       210\n",
      "\n",
      "    accuracy                           0.81      9180\n",
      "   macro avg       0.82      0.81      0.81      9180\n",
      "weighted avg       0.82      0.81      0.81      9180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics for each class\n",
    "print(\"EVALUATION ON TESTING DATA\")\n",
    "print(classification_report(ytest, test_predictions_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce623646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import PCA\n",
    "#dfset3=b1.copy()\n",
    "#pca = PCA(n_components=50)\n",
    "#pca_result = pca.fit_transform(dfset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75e9930d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45900, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "x=pca_result\n",
    "x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.2)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4aed0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=b1.iloc[:,0:1701]\n",
    "#y=y.astype(int)\n",
    "#xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.2)\n",
    "#print(unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "300507c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 99, 8)             136       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 49, 8)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 49, 8)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 55)                21615     \n",
      "=================================================================\n",
      "Total params: 21,799\n",
      "Trainable params: 21,799\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 3.1341 - accuracy: 0.3530 - val_loss: 1.4886 - val_accuracy: 0.7768\n",
      "Epoch 2/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 1.1888 - accuracy: 0.7545 - val_loss: 0.8626 - val_accuracy: 0.8103\n",
      "Epoch 3/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.9378 - accuracy: 0.7883 - val_loss: 0.7878 - val_accuracy: 0.8230\n",
      "Epoch 4/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.8731 - accuracy: 0.8015 - val_loss: 0.7586 - val_accuracy: 0.8238\n",
      "Epoch 5/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.8398 - accuracy: 0.8047 - val_loss: 0.7455 - val_accuracy: 0.8283\n",
      "Epoch 6/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.8142 - accuracy: 0.8118 - val_loss: 0.7419 - val_accuracy: 0.8267\n",
      "Epoch 7/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7984 - accuracy: 0.8120 - val_loss: 0.7345 - val_accuracy: 0.8291\n",
      "Epoch 8/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7848 - accuracy: 0.8135 - val_loss: 0.7288 - val_accuracy: 0.8286- accu\n",
      "Epoch 9/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7766 - accuracy: 0.8174 - val_loss: 0.7297 - val_accuracy: 0.8302\n",
      "Epoch 10/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7709 - accuracy: 0.8169 - val_loss: 0.7344 - val_accuracy: 0.8297\n",
      "Epoch 11/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7661 - accuracy: 0.8184 - val_loss: 0.7331 - val_accuracy: 0.8291\n",
      "Epoch 12/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7549 - accuracy: 0.8193 - val_loss: 0.7288 - val_accuracy: 0.8298\n",
      "Epoch 13/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7570 - accuracy: 0.8168 - val_loss: 0.7365 - val_accuracy: 0.8275\n",
      "Epoch 14/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7459 - accuracy: 0.8208 - val_loss: 0.7265 - val_accuracy: 0.8280\n",
      "Epoch 15/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7448 - accuracy: 0.8208 - val_loss: 0.7339 - val_accuracy: 0.8309\n",
      "Epoch 16/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7426 - accuracy: 0.8196 - val_loss: 0.7348 - val_accuracy: 0.8288\n",
      "Epoch 17/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7387 - accuracy: 0.8214 - val_loss: 0.7345 - val_accuracy: 0.8278\n",
      "Epoch 18/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7331 - accuracy: 0.8230 - val_loss: 0.7338 - val_accuracy: 0.8287\n",
      "Epoch 19/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7345 - accuracy: 0.8215 - val_loss: 0.7341 - val_accuracy: 0.8279\n",
      "Epoch 20/50\n",
      "459/459 [==============================] - 2s 3ms/step - loss: 0.7288 - accuracy: 0.8231 - val_loss: 0.7323 - val_accuracy: 0.8292\n",
      "Epoch 21/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7276 - accuracy: 0.8241 - val_loss: 0.7297 - val_accuracy: 0.8283\n",
      "Epoch 22/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7290 - accuracy: 0.8234 - val_loss: 0.7342 - val_accuracy: 0.8297\n",
      "Epoch 23/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7242 - accuracy: 0.8235 - val_loss: 0.7343 - val_accuracy: 0.8278\n",
      "Epoch 24/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7207 - accuracy: 0.8250 - val_loss: 0.7336 - val_accuracy: 0.8286\n",
      "Epoch 25/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7227 - accuracy: 0.8237 - val_loss: 0.7359 - val_accuracy: 0.8276\n",
      "Epoch 26/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7173 - accuracy: 0.8259 - val_loss: 0.7333 - val_accuracy: 0.8284\n",
      "Epoch 27/50\n",
      "459/459 [==============================] - 2s 3ms/step - loss: 0.7178 - accuracy: 0.8242 - val_loss: 0.7318 - val_accuracy: 0.8290\n",
      "Epoch 28/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7137 - accuracy: 0.8252 - val_loss: 0.7374 - val_accuracy: 0.8288\n",
      "Epoch 29/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7149 - accuracy: 0.8241 - val_loss: 0.7312 - val_accuracy: 0.8295\n",
      "Epoch 30/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7135 - accuracy: 0.8262 - val_loss: 0.7289 - val_accuracy: 0.8283\n",
      "Epoch 31/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7042 - accuracy: 0.8260 - val_loss: 0.7324 - val_accuracy: 0.8286\n",
      "Epoch 32/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7041 - accuracy: 0.8274 - val_loss: 0.7315 - val_accuracy: 0.8291\n",
      "Epoch 33/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7068 - accuracy: 0.8258 - val_loss: 0.7348 - val_accuracy: 0.8267\n",
      "Epoch 34/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7042 - accuracy: 0.8265 - val_loss: 0.7258 - val_accuracy: 0.8290\n",
      "Epoch 35/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7007 - accuracy: 0.8298 - val_loss: 0.7308 - val_accuracy: 0.8287\n",
      "Epoch 36/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.7069 - accuracy: 0.8265 - val_loss: 0.7345 - val_accuracy: 0.8268\n",
      "Epoch 37/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6986 - accuracy: 0.8264 - val_loss: 0.7286 - val_accuracy: 0.8287\n",
      "Epoch 38/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6976 - accuracy: 0.8296 - val_loss: 0.7355 - val_accuracy: 0.8283\n",
      "Epoch 39/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6978 - accuracy: 0.8291 - val_loss: 0.7298 - val_accuracy: 0.8279\n",
      "Epoch 40/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.8280 - val_loss: 0.7247 - val_accuracy: 0.8265\n",
      "Epoch 41/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.8303 - val_loss: 0.7272 - val_accuracy: 0.8284\n",
      "Epoch 42/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6833 - accuracy: 0.8318 - val_loss: 0.7310 - val_accuracy: 0.8290\n",
      "Epoch 43/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.8277 - val_loss: 0.7273 - val_accuracy: 0.8269\n",
      "Epoch 44/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.8285 - val_loss: 0.7242 - val_accuracy: 0.8275\n",
      "Epoch 45/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6830 - accuracy: 0.8313 - val_loss: 0.7272 - val_accuracy: 0.8303\n",
      "Epoch 46/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6859 - accuracy: 0.8311 - val_loss: 0.7296 - val_accuracy: 0.8299\n",
      "Epoch 47/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.8288 - val_loss: 0.7303 - val_accuracy: 0.8292\n",
      "Epoch 48/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6857 - accuracy: 0.8302 - val_loss: 0.7300 - val_accuracy: 0.8292\n",
      "Epoch 49/50\n",
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.8277 - val_loss: 0.7279 - val_accuracy: 0.8279\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459/459 [==============================] - 1s 3ms/step - loss: 0.6815 - accuracy: 0.8316 - val_loss: 0.7303 - val_accuracy: 0.8279\n",
      "   1/1148 [..............................] - ETA: 0s - loss: 0.1936 - accuracy: 0.9375WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0017s). Check your callbacks.\n",
      "1148/1148 [==============================] - 1s 1ms/step - loss: 0.5635 - accuracy: 0.8634\n",
      "Loss: 0.5635262131690979  Accuracy: 0.8634259104728699\n",
      "--- 69.18653106689453 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(16, 2, activation=\"relu\", input_shape=(100,1)))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(55, activation = 'softmax'))\n",
    "start_time = time.time()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "model.summary()\n",
    "model.fit(xtrain, ytrain, batch_size=64,epochs=50,  validation_split=.2, verbose=1)\n",
    "\n",
    "acc = model.evaluate(xtrain, ytrain)\n",
    "print(\"Loss:\", acc[0], \" Accuracy:\", acc[1])\n",
    "\n",
    "#pred = model.predict(xtest)\n",
    "#pred_y = pred.argmax(axis=-1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "933c01f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 0s 1ms/step - loss: 0.7731 - accuracy: 0.8204\n",
      "[0.7730671763420105, 0.8203703761100769]\n",
      "Test results - Loss: 0.7730671763420105 - Accuracy: 0.8203703761100769%\n",
      "--- 0.40583229064941406 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#Test the model after training\n",
    "start_time=time.time()\n",
    "test_results = model.evaluate(xtest, ytest, verbose=1)\n",
    "print(test_results)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9193a174",
   "metadata": {},
   "source": [
    "### Saving xtrain, xtest, ytrain, ytest for CNN-LSTM algorthm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1c801e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=b1.iloc[:,0:1701]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size=0.2,random_state=0)\n",
    "X_train=pd.DataFrame(X_train)\n",
    "X_test=pd.DataFrame(X_test)\n",
    "X_train.to_csv(\"c:/Users/eacun/NRL/NRLDataset/train/Subs/X_train.csv\",index=False,header=False)\n",
    "X_test.to_csv(\"c:/Users/eacun/NRL/NRLDataset/test/Subs/X_test.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98d9e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('c:/Users/eacun/NRL/NRLDataset/train/y_train.csv', ytrain) \n",
    "np.savetxt('c:/Users/eacun/NRL/NRLDataset/test/y_test.csv', ytest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1f6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
