{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation using autoencoders\n",
    "### 40 analytes dataset\n",
    "#### Edgar Acuna\n",
    "#### July 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataframe: (18000, 1701)\n"
     ]
    }
   ],
   "source": [
    "df1=pd.read_csv(\"c://onr2020/NRLset1_part1.csv\",header=None)\n",
    "df2=pd.read_csv(\"c://onr2020/NRLset1_part2.csv\",header=None)\n",
    "df3=pd.read_csv(\"c://onr2020/NRLset1_part3.csv\",header=None)\n",
    "df4=pd.read_csv(\"c://onr2020/NRLset1_part4.csv\",header=None)\n",
    "df5=pd.read_csv(\"c://onr2020/NRLset1_part5.csv\",header=None)\n",
    "df6=pd.read_csv(\"c://onr2020/NRLset1_part6.csv\",header=None)\n",
    "df7=pd.read_csv(\"c://onr2020/NRLset1_part7.csv\",header=None)\n",
    "df8=pd.read_csv(\"c://onr2020/NRLset1_part8.csv\",header=None)\n",
    "y=pd.read_csv(\"c://onr2020/labels.csv\",header=None)\n",
    "ys=pd.read_csv(\"c://onr2020/substrateIDs.csv\",header=None)\n",
    "subs=pd.read_csv(\"c://onr2020/substrates.csv\",header=None)\n",
    "dfset1=pd.concat([df1,df2,df3,df4,df5,df6,df7,df8],ignore_index=True)\n",
    "print('Size of the dataframe: {}'.format(dfset1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "      <th>Analyte</th>\n",
       "      <th>substrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.041418</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.042198</td>\n",
       "      <td>0.042688</td>\n",
       "      <td>0.042924</td>\n",
       "      <td>0.042274</td>\n",
       "      <td>0.042542</td>\n",
       "      <td>0.042663</td>\n",
       "      <td>0.042715</td>\n",
       "      <td>0.042664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053091</td>\n",
       "      <td>0.053140</td>\n",
       "      <td>0.053249</td>\n",
       "      <td>0.053325</td>\n",
       "      <td>0.053364</td>\n",
       "      <td>0.053356</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.055986</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.694710</td>\n",
       "      <td>0.695840</td>\n",
       "      <td>0.697180</td>\n",
       "      <td>0.706530</td>\n",
       "      <td>0.703970</td>\n",
       "      <td>0.709130</td>\n",
       "      <td>0.711480</td>\n",
       "      <td>0.711330</td>\n",
       "      <td>0.712370</td>\n",
       "      <td>0.712980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753630</td>\n",
       "      <td>0.754490</td>\n",
       "      <td>0.752980</td>\n",
       "      <td>0.755760</td>\n",
       "      <td>0.752910</td>\n",
       "      <td>0.755320</td>\n",
       "      <td>0.753030</td>\n",
       "      <td>0.753230</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.048978</td>\n",
       "      <td>0.048432</td>\n",
       "      <td>0.047685</td>\n",
       "      <td>0.047086</td>\n",
       "      <td>0.046811</td>\n",
       "      <td>0.046752</td>\n",
       "      <td>0.046624</td>\n",
       "      <td>0.046443</td>\n",
       "      <td>0.046241</td>\n",
       "      <td>0.045999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258910</td>\n",
       "      <td>0.258210</td>\n",
       "      <td>0.257440</td>\n",
       "      <td>0.256580</td>\n",
       "      <td>0.255700</td>\n",
       "      <td>0.254800</td>\n",
       "      <td>0.254160</td>\n",
       "      <td>0.257110</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.039762</td>\n",
       "      <td>0.039495</td>\n",
       "      <td>0.038982</td>\n",
       "      <td>0.038339</td>\n",
       "      <td>0.037769</td>\n",
       "      <td>0.037301</td>\n",
       "      <td>0.036799</td>\n",
       "      <td>0.036316</td>\n",
       "      <td>0.035921</td>\n",
       "      <td>0.035612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256160</td>\n",
       "      <td>0.255370</td>\n",
       "      <td>0.254540</td>\n",
       "      <td>0.253720</td>\n",
       "      <td>0.252880</td>\n",
       "      <td>0.251970</td>\n",
       "      <td>0.251070</td>\n",
       "      <td>0.250190</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022387</td>\n",
       "      <td>0.022508</td>\n",
       "      <td>0.022091</td>\n",
       "      <td>0.023054</td>\n",
       "      <td>0.023010</td>\n",
       "      <td>0.022740</td>\n",
       "      <td>0.023889</td>\n",
       "      <td>0.023936</td>\n",
       "      <td>0.023464</td>\n",
       "      <td>0.024810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024636</td>\n",
       "      <td>0.022298</td>\n",
       "      <td>0.023536</td>\n",
       "      <td>0.025714</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>0.025062</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>0.023901</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1703 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.041418  0.041621  0.042198  0.042688  0.042924  0.042274  0.042542   \n",
       "1  0.694710  0.695840  0.697180  0.706530  0.703970  0.709130  0.711480   \n",
       "2  0.048978  0.048432  0.047685  0.047086  0.046811  0.046752  0.046624   \n",
       "3  0.039762  0.039495  0.038982  0.038339  0.037769  0.037301  0.036799   \n",
       "4  0.022387  0.022508  0.022091  0.023054  0.023010  0.022740  0.023889   \n",
       "\n",
       "          7         8         9  ...      1693      1694      1695      1696  \\\n",
       "0  0.042663  0.042715  0.042664  ...  0.053091  0.053140  0.053249  0.053325   \n",
       "1  0.711330  0.712370  0.712980  ...  0.753630  0.754490  0.752980  0.755760   \n",
       "2  0.046443  0.046241  0.045999  ...  0.258910  0.258210  0.257440  0.256580   \n",
       "3  0.036316  0.035921  0.035612  ...  0.256160  0.255370  0.254540  0.253720   \n",
       "4  0.023936  0.023464  0.024810  ...  0.024636  0.022298  0.023536  0.025714   \n",
       "\n",
       "       1697      1698      1699      1700  Analyte  substrate  \n",
       "0  0.053364  0.053356  0.053500  0.055986       26          2  \n",
       "1  0.752910  0.755320  0.753030  0.753230       18          3  \n",
       "2  0.255700  0.254800  0.254160  0.257110        8          7  \n",
       "3  0.252880  0.251970  0.251070  0.250190       32          7  \n",
       "4  0.025306  0.025062  0.023609  0.023901       30          1  \n",
       "\n",
       "[5 rows x 1703 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfset2=dfset1.copy()\n",
    "dfset2['Analyte']=y\n",
    "dfset2['substrate']=ys\n",
    "dfsub1=dfset2[dfset2['substrate']==1]\n",
    "dfsub2=dfset2[dfset2['substrate']==2]\n",
    "dfsub3=dfset2[dfset2['substrate']==3]\n",
    "dfsub4=dfset2[dfset2['substrate']==4]\n",
    "dfsub5=dfset2[dfset2['substrate']==5]\n",
    "dfsub6=dfset2[dfset2['substrate']==6]\n",
    "dfsub7=dfset2[dfset2['substrate']==7]\n",
    "dfsub8=dfset2[dfset2['substrate']==8]\n",
    "dfsub9=dfset2[dfset2['substrate']==9]\n",
    "dfset2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the nine substrates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsub1=dfset2[dfset2['substrate']==1]\n",
    "dfsub2=dfset2[dfset2['substrate']==2]\n",
    "dfsub3=dfset2[dfset2['substrate']==3]\n",
    "dfsub4=dfset2[dfset2['substrate']==4]\n",
    "dfsub5=dfset2[dfset2['substrate']==5]\n",
    "dfsub6=dfset2[dfset2['substrate']==6]\n",
    "dfsub7=dfset2[dfset2['substrate']==7]\n",
    "dfsub8=dfset2[dfset2['substrate']==8]\n",
    "dfsub9=dfset2[dfset2['substrate']==9]\n",
    "#dfset1=pd.DataFrame(dfset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfsub1=dfsub1.iloc[:,0:1701]\n",
    "mdfsub2=dfsub2.iloc[:,0:1701]\n",
    "mdfsub3=dfsub3.iloc[:,0:1701]\n",
    "mdfsub4=dfsub4.iloc[:,0:1701]\n",
    "mdfsub5=dfsub5.iloc[:,0:1701]\n",
    "mdfsub6=dfsub6.iloc[:,0:1701]\n",
    "mdfsub7=dfsub7.iloc[:,0:1701]\n",
    "mdfsub8=dfsub8.iloc[:,0:1701]\n",
    "mdfsub9=dfsub9.iloc[:,0:1701]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=subs.loc[0,:]\n",
    "s2=subs.loc[1,:]\n",
    "s3=subs.loc[2,:]\n",
    "s4=subs.loc[3,:]\n",
    "s5=subs.loc[4,:]\n",
    "s6=subs.loc[5,:]\n",
    "s7=subs.loc[6,:]\n",
    "s8=subs.loc[7,:]\n",
    "s9=subs.loc[8,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modsub1=mdfsub1.apply(lambda x : x -(np.sum(np.array(x)*np.array(s1))/np.sum(np.array(s1)*np.array(s1)))*s1,axis=1)\n",
    "modsub2=mdfsub2.apply(lambda x : x -(np.sum(np.array(x)*np.array(s2))/np.sum(np.array(s2)*np.array(s2)))*s2,axis=1)\n",
    "modsub3=mdfsub3.apply(lambda x : x -(np.sum(np.array(x)*np.array(s3))/np.sum(np.array(s3)*np.array(s3)))*s3,axis=1)\n",
    "modsub4=mdfsub4.apply(lambda x : x -(np.sum(np.array(x)*np.array(s4))/np.sum(np.array(s4)*np.array(s4)))*s4,axis=1)\n",
    "modsub5=mdfsub5.apply(lambda x : x -(np.sum(np.array(x)*np.array(s5))/np.sum(np.array(s5)*np.array(s5)))*s5,axis=1)\n",
    "modsub6=mdfsub6.apply(lambda x : x -(np.sum(np.array(x)*np.array(s6))/np.sum(np.array(s6)*np.array(s6)))*s6,axis=1)\n",
    "modsub7=mdfsub7.apply(lambda x : x -(np.sum(np.array(x)*np.array(s7))/np.sum(np.array(s7)*np.array(s7)))*s7,axis=1)\n",
    "modsub8=mdfsub8.apply(lambda x : x -(np.sum(np.array(x)*np.array(s8))/np.sum(np.array(s8)*np.array(s8)))*s8,axis=1)\n",
    "modsub9=mdfsub9.apply(lambda x : x -(np.sum(np.array(x)*np.array(s9))/np.sum(np.array(s9)*np.array(s9)))*s9,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data centered by substrates background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1691</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.006244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001858</td>\n",
       "      <td>-0.002531</td>\n",
       "      <td>-0.002143</td>\n",
       "      <td>-0.004422</td>\n",
       "      <td>-0.003123</td>\n",
       "      <td>-0.000884</td>\n",
       "      <td>-0.001225</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>-0.002792</td>\n",
       "      <td>-0.002432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.005471</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000258</td>\n",
       "      <td>-0.000582</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.003941</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004245</td>\n",
       "      <td>-0.001915</td>\n",
       "      <td>-0.005149</td>\n",
       "      <td>-0.001717</td>\n",
       "      <td>-0.003814</td>\n",
       "      <td>-0.003100</td>\n",
       "      <td>-0.000355</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>-0.005481</td>\n",
       "      <td>-0.000857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.002496</td>\n",
       "      <td>0.016867</td>\n",
       "      <td>0.014204</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.003649</td>\n",
       "      <td>0.006143</td>\n",
       "      <td>-0.015160</td>\n",
       "      <td>0.004201</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>-0.018040</td>\n",
       "      <td>-0.018488</td>\n",
       "      <td>-0.006529</td>\n",
       "      <td>0.009548</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>0.016369</td>\n",
       "      <td>0.007582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004576</td>\n",
       "      <td>-0.004550</td>\n",
       "      <td>-0.004524</td>\n",
       "      <td>-0.004502</td>\n",
       "      <td>-0.004476</td>\n",
       "      <td>-0.004446</td>\n",
       "      <td>-0.004411</td>\n",
       "      <td>-0.004373</td>\n",
       "      <td>-0.004332</td>\n",
       "      <td>-0.004301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.001097</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004655</td>\n",
       "      <td>-0.006438</td>\n",
       "      <td>-0.005102</td>\n",
       "      <td>-0.005563</td>\n",
       "      <td>-0.004804</td>\n",
       "      <td>-0.005517</td>\n",
       "      <td>-0.004411</td>\n",
       "      <td>-0.005738</td>\n",
       "      <td>-0.005031</td>\n",
       "      <td>-0.005584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6     \\\n",
       "4   0.002296  0.002603  0.002368  0.003507  0.003637  0.003537  0.004850   \n",
       "5  -0.005471  0.003865  0.000351 -0.000060 -0.000258 -0.000582  0.002557   \n",
       "8  -0.002496  0.016867  0.014204  0.005327  0.003649  0.006143 -0.015160   \n",
       "28  0.001313  0.001443  0.001536  0.001612  0.001687  0.001768  0.001866   \n",
       "46 -0.001097  0.000040  0.001125  0.002056  0.001685  0.002006  0.000228   \n",
       "\n",
       "        7         8         9     ...      1691      1692      1693      1694  \\\n",
       "4   0.005059  0.004744  0.006244  ... -0.001858 -0.002531 -0.002143 -0.004422   \n",
       "5   0.003072  0.000961  0.003941  ... -0.004245 -0.001915 -0.005149 -0.001717   \n",
       "8   0.004201  0.003791  0.004599  ...  0.002574 -0.018040 -0.018488 -0.006529   \n",
       "28  0.001979  0.002079  0.002155  ... -0.004576 -0.004550 -0.004524 -0.004502   \n",
       "46  0.001179  0.001835  0.002005  ... -0.004655 -0.006438 -0.005102 -0.005563   \n",
       "\n",
       "        1695      1696      1697      1698      1699      1700  \n",
       "4  -0.003123 -0.000884 -0.001225 -0.001406 -0.002792 -0.002432  \n",
       "5  -0.003814 -0.003100 -0.000355 -0.000540 -0.005481 -0.000857  \n",
       "8   0.009548  0.004694  0.000399  0.018158  0.016369  0.007582  \n",
       "28 -0.004476 -0.004446 -0.004411 -0.004373 -0.004332 -0.004301  \n",
       "46 -0.004804 -0.005517 -0.004411 -0.005738 -0.005031 -0.005584  \n",
       "\n",
       "[5 rows x 1701 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf=[modsub1,modsub2,modsub3,modsub4,modsub5,modsub6,modsub7,modsub8,modsub9]\n",
    "cent_subs=pd.concat(subdf)\n",
    "cent_subs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1691</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.006244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001858</td>\n",
       "      <td>-0.002531</td>\n",
       "      <td>-0.002143</td>\n",
       "      <td>-0.004422</td>\n",
       "      <td>-0.003123</td>\n",
       "      <td>-0.000884</td>\n",
       "      <td>-0.001225</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>-0.002792</td>\n",
       "      <td>-0.002432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.005471</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000258</td>\n",
       "      <td>-0.000582</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.003941</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004245</td>\n",
       "      <td>-0.001915</td>\n",
       "      <td>-0.005149</td>\n",
       "      <td>-0.001717</td>\n",
       "      <td>-0.003814</td>\n",
       "      <td>-0.003100</td>\n",
       "      <td>-0.000355</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>-0.005481</td>\n",
       "      <td>-0.000857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.002496</td>\n",
       "      <td>0.016867</td>\n",
       "      <td>0.014204</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.003649</td>\n",
       "      <td>0.006143</td>\n",
       "      <td>-0.015160</td>\n",
       "      <td>0.004201</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>-0.018040</td>\n",
       "      <td>-0.018488</td>\n",
       "      <td>-0.006529</td>\n",
       "      <td>0.009548</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>0.016369</td>\n",
       "      <td>0.007582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004576</td>\n",
       "      <td>-0.004550</td>\n",
       "      <td>-0.004524</td>\n",
       "      <td>-0.004502</td>\n",
       "      <td>-0.004476</td>\n",
       "      <td>-0.004446</td>\n",
       "      <td>-0.004411</td>\n",
       "      <td>-0.004373</td>\n",
       "      <td>-0.004332</td>\n",
       "      <td>-0.004301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.001097</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004655</td>\n",
       "      <td>-0.006438</td>\n",
       "      <td>-0.005102</td>\n",
       "      <td>-0.005563</td>\n",
       "      <td>-0.004804</td>\n",
       "      <td>-0.005517</td>\n",
       "      <td>-0.004411</td>\n",
       "      <td>-0.005738</td>\n",
       "      <td>-0.005031</td>\n",
       "      <td>-0.005584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6     \\\n",
       "4   0.002296  0.002603  0.002368  0.003507  0.003637  0.003537  0.004850   \n",
       "5  -0.005471  0.003865  0.000351 -0.000060 -0.000258 -0.000582  0.002557   \n",
       "8  -0.002496  0.016867  0.014204  0.005327  0.003649  0.006143 -0.015160   \n",
       "28  0.001313  0.001443  0.001536  0.001612  0.001687  0.001768  0.001866   \n",
       "46 -0.001097  0.000040  0.001125  0.002056  0.001685  0.002006  0.000228   \n",
       "\n",
       "        7         8         9     ...      1691      1692      1693      1694  \\\n",
       "4   0.005059  0.004744  0.006244  ... -0.001858 -0.002531 -0.002143 -0.004422   \n",
       "5   0.003072  0.000961  0.003941  ... -0.004245 -0.001915 -0.005149 -0.001717   \n",
       "8   0.004201  0.003791  0.004599  ...  0.002574 -0.018040 -0.018488 -0.006529   \n",
       "28  0.001979  0.002079  0.002155  ... -0.004576 -0.004550 -0.004524 -0.004502   \n",
       "46  0.001179  0.001835  0.002005  ... -0.004655 -0.006438 -0.005102 -0.005563   \n",
       "\n",
       "        1695      1696      1697      1698      1699      1700  \n",
       "4  -0.003123 -0.000884 -0.001225 -0.001406 -0.002792 -0.002432  \n",
       "5  -0.003814 -0.003100 -0.000355 -0.000540 -0.005481 -0.000857  \n",
       "8   0.009548  0.004694  0.000399  0.018158  0.016369  0.007582  \n",
       "28 -0.004476 -0.004446 -0.004411 -0.004373 -0.004332 -0.004301  \n",
       "46 -0.004804 -0.005517 -0.004411 -0.005738 -0.005031 -0.005584  \n",
       "\n",
       "[5 rows x 1701 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "b=cent_subs.iloc[:,0:1701]\n",
    "b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1691</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.010072</td>\n",
       "      <td>0.010446</td>\n",
       "      <td>0.010157</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>0.014528</td>\n",
       "      <td>0.013623</td>\n",
       "      <td>0.017932</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005337</td>\n",
       "      <td>-0.007268</td>\n",
       "      <td>-0.006155</td>\n",
       "      <td>-0.012700</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>-0.002538</td>\n",
       "      <td>-0.003519</td>\n",
       "      <td>-0.004037</td>\n",
       "      <td>-0.008019</td>\n",
       "      <td>-0.006984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.016333</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>-0.000770</td>\n",
       "      <td>-0.001737</td>\n",
       "      <td>0.007633</td>\n",
       "      <td>0.009170</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.011766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012672</td>\n",
       "      <td>-0.005718</td>\n",
       "      <td>-0.015373</td>\n",
       "      <td>-0.005127</td>\n",
       "      <td>-0.011386</td>\n",
       "      <td>-0.009255</td>\n",
       "      <td>-0.001060</td>\n",
       "      <td>-0.001612</td>\n",
       "      <td>-0.016363</td>\n",
       "      <td>-0.002559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.005348</td>\n",
       "      <td>0.036134</td>\n",
       "      <td>0.030431</td>\n",
       "      <td>0.011412</td>\n",
       "      <td>0.007818</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>-0.032478</td>\n",
       "      <td>0.008999</td>\n",
       "      <td>0.008123</td>\n",
       "      <td>0.009853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>-0.038647</td>\n",
       "      <td>-0.039608</td>\n",
       "      <td>-0.013988</td>\n",
       "      <td>0.020456</td>\n",
       "      <td>0.010057</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.038902</td>\n",
       "      <td>0.035069</td>\n",
       "      <td>0.016243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.004714</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.005790</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.007465</td>\n",
       "      <td>0.007738</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016436</td>\n",
       "      <td>-0.016340</td>\n",
       "      <td>-0.016247</td>\n",
       "      <td>-0.016168</td>\n",
       "      <td>-0.016075</td>\n",
       "      <td>-0.015967</td>\n",
       "      <td>-0.015840</td>\n",
       "      <td>-0.015707</td>\n",
       "      <td>-0.015558</td>\n",
       "      <td>-0.015445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.003245</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>0.004984</td>\n",
       "      <td>0.005934</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>0.005932</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013769</td>\n",
       "      <td>-0.019043</td>\n",
       "      <td>-0.015090</td>\n",
       "      <td>-0.016453</td>\n",
       "      <td>-0.014209</td>\n",
       "      <td>-0.016319</td>\n",
       "      <td>-0.013047</td>\n",
       "      <td>-0.016972</td>\n",
       "      <td>-0.014881</td>\n",
       "      <td>-0.016518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6     \\\n",
       "4   0.006593  0.007476  0.006800  0.010072  0.010446  0.010157  0.013928   \n",
       "5  -0.016333  0.011538  0.001048 -0.000179 -0.000770 -0.001737  0.007633   \n",
       "8  -0.005348  0.036134  0.030431  0.011412  0.007818  0.013162 -0.032478   \n",
       "28  0.004714  0.005182  0.005515  0.005790  0.006058  0.006349  0.006700   \n",
       "46 -0.003245  0.000118  0.003326  0.006083  0.004984  0.005934  0.000673   \n",
       "\n",
       "        7         8         9     ...      1691      1692      1693      1694  \\\n",
       "4   0.014528  0.013623  0.017932  ... -0.005337 -0.007268 -0.006155 -0.012700   \n",
       "5   0.009170  0.002870  0.011766  ... -0.012672 -0.005718 -0.015373 -0.005127   \n",
       "8   0.008999  0.008123  0.009853  ...  0.005515 -0.038647 -0.039608 -0.013988   \n",
       "28  0.007108  0.007465  0.007738  ... -0.016436 -0.016340 -0.016247 -0.016168   \n",
       "46  0.003487  0.005428  0.005932  ... -0.013769 -0.019043 -0.015090 -0.016453   \n",
       "\n",
       "        1695      1696      1697      1698      1699      1700  \n",
       "4  -0.008968 -0.002538 -0.003519 -0.004037 -0.008019 -0.006984  \n",
       "5  -0.011386 -0.009255 -0.001060 -0.001612 -0.016363 -0.002559  \n",
       "8   0.020456  0.010057  0.000856  0.038902  0.035069  0.016243  \n",
       "28 -0.016075 -0.015967 -0.015840 -0.015707 -0.015558 -0.015445  \n",
       "46 -0.014209 -0.016319 -0.013047 -0.016972 -0.014881 -0.016518  \n",
       "\n",
       "[5 rows x 1701 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize(b)\n",
    "b1=b.apply(lambda x: x/(x**2).sum()**.5, axis=1)\n",
    "b1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the data for autoenconders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45711583, 0.48938046, 0.48649315, ..., 0.42916987, 0.48260493,\n",
       "        0.43545418],\n",
       "       [0.32434561, 0.51334761, 0.45458234, ..., 0.44226193, 0.43586269,\n",
       "        0.4644868 ],\n",
       "       [0.38796258, 0.65849522, 0.61759402, ..., 0.66098456, 0.72400473,\n",
       "        0.58786694],\n",
       "       ...,\n",
       "       [0.60793619, 0.60722394, 0.40874603, ..., 0.30006901, 0.39184086,\n",
       "        0.38102345],\n",
       "       [0.811848  , 0.83086698, 0.65052654, ..., 0.36572404, 0.4499319 ,\n",
       "        0.40042738],\n",
       "       [0.60435644, 0.61476153, 0.41242006, ..., 0.37122376, 0.46287626,\n",
       "        0.43094634]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense\n",
    "from sklearn.preprocessing import  StandardScaler, MinMaxScaler\n",
    "trainx =b1.iloc[:,0:1701]\n",
    "trainx=trainx.to_numpy()\n",
    "scaler = MinMaxScaler()\n",
    "train_x= scaler.fit_transform(trainx)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1701)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                108928    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1701)              56133     \n",
      "=================================================================\n",
      "Total params: 168,197\n",
      "Trainable params: 168,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# No of Neurons in each Layer \n",
    "nb_epoch = 40\n",
    "batch_size = 50\n",
    "input_dim = train_x.shape[1] #num of columns, 1701\n",
    "encoding_dim = 64\n",
    "hidden_dim = int(encoding_dim / 2) #i.e. 32\n",
    "#decoding_dim = 64\n",
    "learning_rate = 1e-7\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
    "encoder = Dense(hidden_dim, activation=\"relu\")(encoder)\n",
    "decoder = Dense(hidden_dim, activation='tanh')(encoder)\n",
    "#decoder = Dense(decoding_dim, activation='relu')(decoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "288/288 [==============================] - 1s 3ms/step - loss: 0.0943 - val_loss: 0.0915\n",
      "Epoch 2/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0793 - val_loss: 0.0812\n",
      "Epoch 3/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0704 - val_loss: 0.0776\n",
      "Epoch 4/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0675 - val_loss: 0.0724\n",
      "Epoch 5/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0634 - val_loss: 0.0672\n",
      "Epoch 6/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0594 - val_loss: 0.0622\n",
      "Epoch 7/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0560 - val_loss: 0.0592\n",
      "Epoch 8/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0529 - val_loss: 0.0553\n",
      "Epoch 9/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0507 - val_loss: 0.0540\n",
      "Epoch 10/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0495 - val_loss: 0.0527\n",
      "Epoch 11/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0483 - val_loss: 0.0520\n",
      "Epoch 12/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0472 - val_loss: 0.0496\n",
      "Epoch 13/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0451 - val_loss: 0.0479\n",
      "Epoch 14/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0436 - val_loss: 0.0469\n",
      "Epoch 15/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0430 - val_loss: 0.0466\n",
      "Epoch 16/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0427 - val_loss: 0.0455\n",
      "Epoch 17/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0410 - val_loss: 0.0438\n",
      "Epoch 18/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0397 - val_loss: 0.0419\n",
      "Epoch 19/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0371 - val_loss: 0.0381\n",
      "Epoch 20/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0349 - val_loss: 0.0367\n",
      "Epoch 21/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0335 - val_loss: 0.0353\n",
      "Epoch 22/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0340\n",
      "Epoch 23/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.0334\n",
      "Epoch 24/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0326\n",
      "Epoch 25/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0320\n",
      "Epoch 26/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0316\n",
      "Epoch 27/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0312\n",
      "Epoch 28/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0307\n",
      "Epoch 29/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0298\n",
      "Epoch 30/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0295\n",
      "Epoch 31/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0286\n",
      "Epoch 32/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0279\n",
      "Epoch 33/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0273\n",
      "Epoch 34/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0272\n",
      "Epoch 35/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0262\n",
      "Epoch 36/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0260\n",
      "Epoch 37/40\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.0237 - val_loss: 0.0251\n",
      "Epoch 38/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0244\n",
      "Epoch 39/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0239\n",
      "Epoch 40/40\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0234\n",
      "--- 21.760228633880615 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "autoencoder.compile(optimizer='adam', loss='mse' )\n",
    "\n",
    "history = autoencoder.fit(train_x, train_x, epochs=nb_epoch,batch_size=batch_size, shuffle=True,validation_split=0.2,verbose=1)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "df_history = pd.DataFrame(history.history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46056485 0.4835794  0.48675594 ... 0.40872523 0.49765897 0.        ]\n",
      " [0.44311392 0.45918366 0.45932528 ... 0.39894387 0.48653167 0.        ]\n",
      " [0.41271424 0.42285976 0.42456836 ... 0.41520587 0.50569165 0.        ]\n",
      " ...\n",
      " [0.5500539  0.5616297  0.5536279  ... 0.33776048 0.43143654 0.        ]\n",
      " [0.6398362  0.65822816 0.64466673 ... 0.32757968 0.4172185  0.        ]\n",
      " [0.5767274  0.59481716 0.58550996 ... 0.38992313 0.48328555 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "predictions = autoencoder.predict(train_x)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1691</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.460565</td>\n",
       "      <td>0.483579</td>\n",
       "      <td>0.486756</td>\n",
       "      <td>0.492120</td>\n",
       "      <td>0.558143</td>\n",
       "      <td>0.547319</td>\n",
       "      <td>0.571584</td>\n",
       "      <td>0.539622</td>\n",
       "      <td>0.521876</td>\n",
       "      <td>0.494091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487231</td>\n",
       "      <td>0.490130</td>\n",
       "      <td>0.426272</td>\n",
       "      <td>0.474585</td>\n",
       "      <td>0.462609</td>\n",
       "      <td>0.490855</td>\n",
       "      <td>0.488666</td>\n",
       "      <td>0.408725</td>\n",
       "      <td>0.497659</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.443114</td>\n",
       "      <td>0.459184</td>\n",
       "      <td>0.459325</td>\n",
       "      <td>0.454957</td>\n",
       "      <td>0.511528</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.526783</td>\n",
       "      <td>0.497360</td>\n",
       "      <td>0.486090</td>\n",
       "      <td>0.449556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476967</td>\n",
       "      <td>0.484295</td>\n",
       "      <td>0.415580</td>\n",
       "      <td>0.462455</td>\n",
       "      <td>0.453752</td>\n",
       "      <td>0.482757</td>\n",
       "      <td>0.480060</td>\n",
       "      <td>0.398944</td>\n",
       "      <td>0.486532</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.412714</td>\n",
       "      <td>0.422860</td>\n",
       "      <td>0.424568</td>\n",
       "      <td>0.423803</td>\n",
       "      <td>0.474847</td>\n",
       "      <td>0.461265</td>\n",
       "      <td>0.489338</td>\n",
       "      <td>0.459784</td>\n",
       "      <td>0.450983</td>\n",
       "      <td>0.422398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505846</td>\n",
       "      <td>0.512411</td>\n",
       "      <td>0.441172</td>\n",
       "      <td>0.487830</td>\n",
       "      <td>0.480978</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.500680</td>\n",
       "      <td>0.415206</td>\n",
       "      <td>0.505692</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.473523</td>\n",
       "      <td>0.495420</td>\n",
       "      <td>0.492629</td>\n",
       "      <td>0.488196</td>\n",
       "      <td>0.547281</td>\n",
       "      <td>0.535176</td>\n",
       "      <td>0.559948</td>\n",
       "      <td>0.528346</td>\n",
       "      <td>0.512583</td>\n",
       "      <td>0.479831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459113</td>\n",
       "      <td>0.466331</td>\n",
       "      <td>0.396244</td>\n",
       "      <td>0.445663</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.465793</td>\n",
       "      <td>0.460989</td>\n",
       "      <td>0.384487</td>\n",
       "      <td>0.470583</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.466921</td>\n",
       "      <td>0.488979</td>\n",
       "      <td>0.486755</td>\n",
       "      <td>0.483489</td>\n",
       "      <td>0.541455</td>\n",
       "      <td>0.529812</td>\n",
       "      <td>0.553299</td>\n",
       "      <td>0.521255</td>\n",
       "      <td>0.505341</td>\n",
       "      <td>0.471940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465931</td>\n",
       "      <td>0.473525</td>\n",
       "      <td>0.401898</td>\n",
       "      <td>0.451975</td>\n",
       "      <td>0.441390</td>\n",
       "      <td>0.471358</td>\n",
       "      <td>0.466638</td>\n",
       "      <td>0.389966</td>\n",
       "      <td>0.476114</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.460565  0.483579  0.486756  0.492120  0.558143  0.547319  0.571584   \n",
       "1  0.443114  0.459184  0.459325  0.454957  0.511528  0.502045  0.526783   \n",
       "2  0.412714  0.422860  0.424568  0.423803  0.474847  0.461265  0.489338   \n",
       "3  0.473523  0.495420  0.492629  0.488196  0.547281  0.535176  0.559948   \n",
       "4  0.466921  0.488979  0.486755  0.483489  0.541455  0.529812  0.553299   \n",
       "\n",
       "       7         8         9     ...      1691      1692      1693      1694  \\\n",
       "0  0.539622  0.521876  0.494091  ...  0.487231  0.490130  0.426272  0.474585   \n",
       "1  0.497360  0.486090  0.449556  ...  0.476967  0.484295  0.415580  0.462455   \n",
       "2  0.459784  0.450983  0.422398  ...  0.505846  0.512411  0.441172  0.487830   \n",
       "3  0.528346  0.512583  0.479831  ...  0.459113  0.466331  0.396244  0.445663   \n",
       "4  0.521255  0.505341  0.471940  ...  0.465931  0.473525  0.401898  0.451975   \n",
       "\n",
       "       1695      1696      1697      1698      1699  1700  \n",
       "0  0.462609  0.490855  0.488666  0.408725  0.497659   0.0  \n",
       "1  0.453752  0.482757  0.480060  0.398944  0.486532   0.0  \n",
       "2  0.480978  0.502045  0.500680  0.415206  0.505692   0.0  \n",
       "3  0.435294  0.465793  0.460989  0.384487  0.470583   0.0  \n",
       "4  0.441390  0.471358  0.466638  0.389966  0.476114   0.0  \n",
       "\n",
       "[5 rows x 1701 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=pd.DataFrame(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "augdata=pd.concat([train_x,predictions],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 1701)\n"
     ]
    }
   ],
   "source": [
    "print(augdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=np.array(dfsub1['Analyte'].values.tolist())\n",
    "a2=np.array(dfsub2['Analyte'].values.tolist())\n",
    "a3=np.array(dfsub3['Analyte'].values.tolist())\n",
    "a4=np.array(dfsub4['Analyte'].values.tolist())\n",
    "a5=np.array(dfsub5['Analyte'].values.tolist())\n",
    "a6=np.array(dfsub6['Analyte'].values.tolist())\n",
    "a7=np.array(dfsub7['Analyte'].values.tolist())\n",
    "a8=np.array(dfsub8['Analyte'].values.tolist())\n",
    "a9=np.array(dfsub9['Analyte'].values.tolist())\n",
    "y=np.concatenate((a1,a2,a3,a4,a5,a6,a7,a8,a9),axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "augy=np.concatenate((a1,a2,a3,a4,a5,a6,a7,a8,a9,a1,a2,a3,a4,a5,a6,a7,a8,a9),axis=None)\n",
    "augy=pd.DataFrame(augy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1692</th>\n",
       "      <th>1693</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "      <th>Analyte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35995</th>\n",
       "      <td>0.544029</td>\n",
       "      <td>0.564594</td>\n",
       "      <td>0.562590</td>\n",
       "      <td>0.538200</td>\n",
       "      <td>0.583195</td>\n",
       "      <td>0.587420</td>\n",
       "      <td>0.593123</td>\n",
       "      <td>0.590762</td>\n",
       "      <td>0.563569</td>\n",
       "      <td>0.507713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444787</td>\n",
       "      <td>0.359655</td>\n",
       "      <td>0.426219</td>\n",
       "      <td>0.409119</td>\n",
       "      <td>0.448875</td>\n",
       "      <td>0.438485</td>\n",
       "      <td>0.366499</td>\n",
       "      <td>0.455328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35996</th>\n",
       "      <td>0.615789</td>\n",
       "      <td>0.638487</td>\n",
       "      <td>0.626592</td>\n",
       "      <td>0.603206</td>\n",
       "      <td>0.670327</td>\n",
       "      <td>0.670720</td>\n",
       "      <td>0.693729</td>\n",
       "      <td>0.674622</td>\n",
       "      <td>0.661118</td>\n",
       "      <td>0.609632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412240</td>\n",
       "      <td>0.350868</td>\n",
       "      <td>0.397867</td>\n",
       "      <td>0.386393</td>\n",
       "      <td>0.422903</td>\n",
       "      <td>0.414631</td>\n",
       "      <td>0.337026</td>\n",
       "      <td>0.433557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35997</th>\n",
       "      <td>0.550054</td>\n",
       "      <td>0.561630</td>\n",
       "      <td>0.553628</td>\n",
       "      <td>0.528738</td>\n",
       "      <td>0.583146</td>\n",
       "      <td>0.582664</td>\n",
       "      <td>0.606475</td>\n",
       "      <td>0.588420</td>\n",
       "      <td>0.574775</td>\n",
       "      <td>0.527850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418386</td>\n",
       "      <td>0.347656</td>\n",
       "      <td>0.400304</td>\n",
       "      <td>0.385908</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>0.410556</td>\n",
       "      <td>0.337760</td>\n",
       "      <td>0.431437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35998</th>\n",
       "      <td>0.639836</td>\n",
       "      <td>0.658228</td>\n",
       "      <td>0.644667</td>\n",
       "      <td>0.610123</td>\n",
       "      <td>0.673862</td>\n",
       "      <td>0.678563</td>\n",
       "      <td>0.704107</td>\n",
       "      <td>0.683188</td>\n",
       "      <td>0.679882</td>\n",
       "      <td>0.623255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404263</td>\n",
       "      <td>0.339258</td>\n",
       "      <td>0.384795</td>\n",
       "      <td>0.374255</td>\n",
       "      <td>0.410190</td>\n",
       "      <td>0.403803</td>\n",
       "      <td>0.327580</td>\n",
       "      <td>0.417219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35999</th>\n",
       "      <td>0.576727</td>\n",
       "      <td>0.594817</td>\n",
       "      <td>0.585510</td>\n",
       "      <td>0.559755</td>\n",
       "      <td>0.612796</td>\n",
       "      <td>0.616671</td>\n",
       "      <td>0.628374</td>\n",
       "      <td>0.612385</td>\n",
       "      <td>0.596461</td>\n",
       "      <td>0.537990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473031</td>\n",
       "      <td>0.404669</td>\n",
       "      <td>0.457279</td>\n",
       "      <td>0.445107</td>\n",
       "      <td>0.475039</td>\n",
       "      <td>0.471586</td>\n",
       "      <td>0.389923</td>\n",
       "      <td>0.483286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1702 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "35995  0.544029  0.564594  0.562590  0.538200  0.583195  0.587420  0.593123   \n",
       "35996  0.615789  0.638487  0.626592  0.603206  0.670327  0.670720  0.693729   \n",
       "35997  0.550054  0.561630  0.553628  0.528738  0.583146  0.582664  0.606475   \n",
       "35998  0.639836  0.658228  0.644667  0.610123  0.673862  0.678563  0.704107   \n",
       "35999  0.576727  0.594817  0.585510  0.559755  0.612796  0.616671  0.628374   \n",
       "\n",
       "              7         8         9  ...      1692      1693      1694  \\\n",
       "35995  0.590762  0.563569  0.507713  ...  0.444787  0.359655  0.426219   \n",
       "35996  0.674622  0.661118  0.609632  ...  0.412240  0.350868  0.397867   \n",
       "35997  0.588420  0.574775  0.527850  ...  0.418386  0.347656  0.400304   \n",
       "35998  0.683188  0.679882  0.623255  ...  0.404263  0.339258  0.384795   \n",
       "35999  0.612385  0.596461  0.537990  ...  0.473031  0.404669  0.457279   \n",
       "\n",
       "           1695      1696      1697      1698      1699  1700  Analyte  \n",
       "35995  0.409119  0.448875  0.438485  0.366499  0.455328   0.0       32  \n",
       "35996  0.386393  0.422903  0.414631  0.337026  0.433557   0.0       11  \n",
       "35997  0.385908  0.420700  0.410556  0.337760  0.431437   0.0       26  \n",
       "35998  0.374255  0.410190  0.403803  0.327580  0.417219   0.0        5  \n",
       "35999  0.445107  0.475039  0.471586  0.389923  0.483286   0.0        4  \n",
       "\n",
       "[5 rows x 1702 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augdata['Analyte']=augy\n",
    "augdata.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
      "       35, 36, 37, 38, 39, 40]), array([900, 900, 900, 900, 900, 900, 900, 900, 900, 900, 900, 900, 900,\n",
      "       900, 900, 900, 900, 900, 900, 900, 900, 900, 900, 900, 900, 900,\n",
      "       900, 900, 900, 900, 900, 900, 900, 900, 900, 900, 900, 900, 900,\n",
      "       900], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "y=augy.to_numpy()\n",
    "print(np.unique(y,return_counts=True))\n",
    "y=np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, accuracy=91.29%\n",
      "k=3, accuracy=91.42%\n",
      "k=5, accuracy=91.69%\n",
      "k=7, accuracy=91.75%\n",
      "k=9, accuracy=91.71%\n",
      "k=11, accuracy=91.47%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# We will find by inspection the best k according to the classifier accuracy on the test set\n",
    "accuracies = []\n",
    "X=augdata.iloc[:,0:1701]\n",
    "# We will find by inspection the best k according to the classifier accuracy on the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=0)\n",
    "for k in range(1, 12, 2):\n",
    "    # Entrenar el clasificador  con el valor actual de  `k`\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar los modelos e imprimiendo sus predicciones\n",
    "    score = neigh.score(X_test, y_test)\n",
    "    print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "    accuracies.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import tempfile\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, roc_auc_score\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dimension:\n",
      "(28800, 1701)\n",
      "Test dimension:\n",
      "(7200, 55)\n"
     ]
    }
   ],
   "source": [
    "# Configuration options\n",
    "feature_vector_length = 1701\n",
    "num_classes = 55\n",
    "x=augdata.iloc[:,0:1701].to_numpy()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size=0.2,random_state=0)\n",
    "# Convert target classes to categorical ones\n",
    "ytrain=Y_train-1\n",
    "ytest=Y_test-1\n",
    "Y_train = to_categorical(ytrain, num_classes)\n",
    "Y_test = to_categorical(ytest, num_classes)\n",
    "print('Train dimension:')\n",
    "print(X_train.shape)\n",
    "print('Test dimension:')\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (1701,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 1701)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               510600    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 55)                11055     \n",
      "=================================================================\n",
      "Total params: 581,855\n",
      "Trainable params: 581,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set the input shape\n",
    "input_shape = (feature_vector_length,)\n",
    "print(f'Feature shape: {input_shape}')\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.3, input_shape=input_shape))\n",
    "model.add(Dense(300, input_shape=input_shape, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 3.6024 - accuracy: 0.0525WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0021s). Check your callbacks.\n",
      "154/154 [==============================] - 12s 77ms/step - loss: 3.6024 - accuracy: 0.0525 - val_loss: 3.2999 - val_accuracy: 0.0962\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - 10s 63ms/step - loss: 3.2294 - accuracy: 0.1101 - val_loss: 2.9987 - val_accuracy: 0.1934\n",
      "Epoch 3/100\n",
      "154/154 [==============================] - 8s 51ms/step - loss: 2.9565 - accuracy: 0.1878 - val_loss: 2.6294 - val_accuracy: 0.2649\n",
      "Epoch 4/100\n",
      "154/154 [==============================] - 6s 38ms/step - loss: 2.7243 - accuracy: 0.2406 - val_loss: 2.4488 - val_accuracy: 0.3271\n",
      "Epoch 5/100\n",
      "154/154 [==============================] - 9s 60ms/step - loss: 2.5683 - accuracy: 0.2728 - val_loss: 2.1732 - val_accuracy: 0.4262\n",
      "Epoch 6/100\n",
      "154/154 [==============================] - 15s 94ms/step - loss: 2.4858 - accuracy: 0.2941 - val_loss: 2.1417 - val_accuracy: 0.4344\n",
      "Epoch 7/100\n",
      "154/154 [==============================] - 11s 71ms/step - loss: 2.4186 - accuracy: 0.3138 - val_loss: 2.0246 - val_accuracy: 0.4767\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - 10s 62ms/step - loss: 2.3707 - accuracy: 0.3187 - val_loss: 1.9599 - val_accuracy: 0.4826\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - 11s 72ms/step - loss: 2.3123 - accuracy: 0.3358 - val_loss: 1.9430 - val_accuracy: 0.4642\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - 10s 64ms/step - loss: 2.2813 - accuracy: 0.3451 - val_loss: 1.8694 - val_accuracy: 0.4894\n",
      "Epoch 11/100\n",
      "154/154 [==============================] - 9s 62ms/step - loss: 2.2523 - accuracy: 0.3528 - val_loss: 1.8237 - val_accuracy: 0.5109\n",
      "Epoch 12/100\n",
      "154/154 [==============================] - 10s 65ms/step - loss: 2.2145 - accuracy: 0.3601 - val_loss: 1.8262 - val_accuracy: 0.4906\n",
      "Epoch 13/100\n",
      "154/154 [==============================] - 9s 60ms/step - loss: 2.2064 - accuracy: 0.3572 - val_loss: 1.7687 - val_accuracy: 0.5043\n",
      "Epoch 14/100\n",
      "154/154 [==============================] - 11s 73ms/step - loss: 2.1541 - accuracy: 0.3733 - val_loss: 1.7538 - val_accuracy: 0.5075\n",
      "Epoch 15/100\n",
      "154/154 [==============================] - 11s 69ms/step - loss: 2.1399 - accuracy: 0.3751 - val_loss: 1.7527 - val_accuracy: 0.5172\n",
      "Epoch 16/100\n",
      "154/154 [==============================] - 11s 69ms/step - loss: 2.1099 - accuracy: 0.3834 - val_loss: 1.7179 - val_accuracy: 0.5059\n",
      "Epoch 17/100\n",
      "154/154 [==============================] - 12s 77ms/step - loss: 2.0988 - accuracy: 0.3848 - val_loss: 1.6761 - val_accuracy: 0.5253\n",
      "Epoch 18/100\n",
      "154/154 [==============================] - 11s 68ms/step - loss: 2.0689 - accuracy: 0.3872 - val_loss: 1.6665 - val_accuracy: 0.5398\n",
      "Epoch 19/100\n",
      "154/154 [==============================] - 5s 34ms/step - loss: 2.0579 - accuracy: 0.3968 - val_loss: 1.6734 - val_accuracy: 0.5319\n",
      "Epoch 20/100\n",
      " 43/154 [=======>......................] - ETA: 2s - loss: 2.0331 - accuracy: 0.4062"
     ]
    }
   ],
   "source": [
    "# Configure the model and start training\n",
    "import time\n",
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "baseline_history=model.fit(X_train, Y_train, epochs=100, batch_size=150, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training time:---  %s seconds ---\" % (time.time() - start_time))\n",
    "#Test the model after training\n",
    "start_time=time.time()\n",
    "test_results = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(test_results)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.datasets import load_iris\n",
    "from numpy import unique\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=augdata.iloc[:,0:1701].to_numpy()\n",
    "x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "print(x.shape)\n",
    "y=y-1\n",
    "#print(unique(y))\n",
    "#print(unique(y).sum())\n",
    "\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(32, 3, activation=\"relu\", input_shape=(1701,1)))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(32, 3, activation=\"relu\", input_shape=(1701,1)))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(55, activation = 'softmax'))\n",
    "start_time = time.time()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "model.summary()\n",
    "model = Sequential()\n",
    "baseline_history=model.fit(xtrain, ytrain, epochs=30, batch_size=256, verbose=1, validation_split=0.2)\n",
    "#model.fit(xtrain, ytrain, batch_size=256,epochs=25,  validation_split=.2, verbose=1)\n",
    "acc = model.evaluate(xtrain, ytrain)\n",
    "print(\"Loss:\", acc[0], \" Accuracy:\", acc[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#Test the model after training\n",
    "start_time=time.time()\n",
    "test_results = model.evaluate(xtest, ytest, verbose=1)\n",
    "print(test_results)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
